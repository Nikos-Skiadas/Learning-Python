{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2.1\n",
    "\n",
    "Write a short summary of what the assignment is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for this notebook to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q numpy\n",
    "%pip install -q pandas\n",
    "%pip install -q scikit-learn\n",
    "%pip install -q nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Python library imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import typing\n",
    "import warnings; warnings.filterwarnings(\"ignore\",\n",
    "\tcategory = UserWarning,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backend used for calculations and data manipulation. Mostly `pandas` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific modules for `sklearn`:\n",
    "- `feature_extraction` for transformation of texts into features\n",
    "- `linear_model` for the models used for this assignemt\n",
    "- `pipeline` to assemble the various steps in a single process\n",
    "- `metrics` for making evaluations on known splits (either training or validation)\n",
    "- `model_selection` for handling tuning of hyper-parameters of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.linear_model\n",
    "import sklearn.pipeline\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Natural Language Tool Kit:\n",
    "- `stem` has tools for preprocessing text\n",
    "- `corpus` has language specific content like stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.stem\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nikos/nltk_data...\n",
      "[nltk_data] Downloading package stopwords to /home/nikos/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet'  )  # for lemmatization\n",
    "nltk.download('stopwords')  # for stop word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "This assignment works on an english Twitter dataset with attributes:\n",
    "\n",
    "- `ID`: a unique identifier for each text\n",
    "- `Text`: contains the content of the tweet\n",
    "- `Label`: represents sentiment:\n",
    "  - `1` indicates a _positive_ sentiment\n",
    "  - `0` indicates a _negative_ sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assumes that the following files exist in the same path as this notebook:\n",
    "\n",
    "- `train_dataset.csv` (used for training models)\n",
    "- `val_dataset.csv` (used for tuning models and later training them as well)\n",
    "- `test_dataset.csv` (used for generating predictions for final evaluation)\n",
    "\n",
    "Also, this notebook upon execution, generates a `submission.csv` file with the predictions of our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with a simple function to load each dataset split into a dataframe indexed with the `ID` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "\tsplit: typing.Literal[\n",
    "\t\t\"train\",\n",
    "\t\t\"val\",\n",
    "\t\t\"test\",\n",
    "\t],\n",
    "\troot: str = \"\",\n",
    "\tindex: str = \"ID\",\n",
    "):\n",
    "\treturn pandas.read_csv(os.path.join(f\"{root}\", f\"{split}_dataset.csv\"),\n",
    "\t\tindex_col = index,  # put IDs on index\n",
    "\t\tencoding = \"utf-8\",\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "\n",
    "After inspecting a sample, tweets require a great amount of normalization:\n",
    "\n",
    "- lowercasing\n",
    "- removal of punctuation\n",
    "- removal of accents\n",
    "- removal of stop words\n",
    "- removal of sensitive information like:\n",
    "  - mentions\n",
    "  - hashtags\n",
    "  - emails\n",
    "- lemmatization\n",
    "- stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn` term-frequency-inverse-document-frequency vectorizer supports lowercasing and stop words out of the box, but the later shall be replaced by the NLTK version for english. It also supports a custom preprocessor and tokenizer which are defined below as callables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "This step consists of removing sensitive info and punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "\tdef __call__(self, text: str) -> str:\n",
    "\t\ttext = re.sub(r\"@\\w+\"   , \"\", text)  # mentions\n",
    "\t\ttext = re.sub(r\"#\\w+\"   , \"\", text)  # hashtags\n",
    "\t\ttext = re.sub(r\"\\S+@\\S+\", \"\", text)  # emails\n",
    "\n",
    "\t\treturn text.translate(str.maketrans(\"\", \"\", string.punctuation))  # punctuation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing\n",
    "\n",
    "This step consists of lemmatizing and stemming texts before splitting into (meaningful) tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.lemmatizer = nltk.WordNetLemmatizer()\n",
    "\t\tself.stemmer = nltk.stem.PorterStemmer()\n",
    "\t\tself.tokenizer = nltk.tokenize.TweetTokenizer(\n",
    "\t\t\tpreserve_case = False,\n",
    "\t\t\treduce_len = True,\n",
    "\t\t\tmatch_phone_numbers = True,\n",
    "\t\t\tstrip_handles = True,\n",
    "\t\t)\n",
    "\n",
    "\tdef __call__(self, text: str):\n",
    "\t\treturn [self.stemmer.stem(self.lemmatizer.lemmatize(token))\n",
    "\t\t\tfor token in self.tokenizer.tokenize(text) if token and not token.isdigit()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "The pipeline for this assignment consists of two steps:\n",
    "\n",
    "1. Vectorization of texts (using _term-frequency-inverse-document-frequency_)\n",
    "2. Modelling of the task (_binary sentiment_ estimation of tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer\n",
    "\n",
    "Lets specialize a vectorizer to expose two parameters only:\n",
    "\n",
    "- `max_features`: Usually the vocabulary a vectorizer produces from a corpus is extremely large and unusable due to the curse of dimensionality. Also such vocabulary is ususally sorted by frequency, so trimming terms from the end means ditching rare terms which may not be so bad.\n",
    "- `ngrams_range`: By default a vectorizer will use single-word terms to describe texts. Using overlapping two-word terms (bigrams) though, can lead to better capturing of the text content, but that also leads to much larger vocabularies, making `max_features` all the more necessary. The library supports combination settings with this range, using both unigrams and bigrams for example. As bigrams are generally rarer, they tend to get clipped by `max_features` so it makes little sense to go to trigrams (or more)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer(sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "\n",
    "\tdef __init__(self, *,\n",
    "\t\tmax_features: int | None = None,  # regularize by reduing curse of dimensionality\n",
    "\t\tngram_range: tuple[\n",
    "\t\t\tint,\n",
    "\t\t\tint,\n",
    "\t\t] = (\n",
    "\t\t\t1,\n",
    "\t\t\t1,\n",
    "\t\t),\n",
    "\t):\n",
    "\t\tsuper().__init__(\n",
    "\t\t#\tinput = \"content\",\n",
    "\t\t\tencoding = \"utf-8\",\n",
    "\t\t#\tdecode_error = \"strict\",\n",
    "\t\t\tstrip_accents = \"unicode\",\n",
    "\t\t\tlowercase = True,\n",
    "\t\t\tpreprocessor = Preprocessor(),\n",
    "\t\t\ttokenizer = Tokenizer(),\n",
    "\t\t#\tanalyzer = \"word\",\n",
    "\t\t\tstop_words = nltk.corpus.stopwords.words(\"english\"),  # messages seem to be in english\n",
    "\t\t#\ttoken_pattern = \"(?u)\\\\b\\\\w\\\\w+\\\\b\",\n",
    "\t\t\tngram_range = ngram_range,  # NOTE: maybe tunable\n",
    "\t\t#\tmax_df = 1.0,\n",
    "\t\t#\tmin_df = 1,\n",
    "\t\t\tmax_features = max_features, # NOTE: tunable\n",
    "\t\t#\tvocabulary = None,\n",
    "\t\t#\tbinary = False,\n",
    "\t\t#\tdtype = numpy.float64,\n",
    "\t\t#\tnorm = \"l2\",\n",
    "\t\t#\tuse_idf = True,\n",
    "\t\t#\tsmooth_idf = True,\n",
    "\t\t#\tsublinear_tf = False,\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seed is fixed at `42` for reproducibility. Notice how lowercasing and accent removal is passed explicitely. Also this is the palce where the stopwords are passed, along with a preprocessor and tokenizer as defined above. This wraps the vectorizer to be used for experiments on this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Lets also specialize the model to be used for this assignment. A logistic regressor is requested and frankly suitable for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(sklearn.linear_model.LogisticRegression):\n",
    "\n",
    "\tdef __init__(self, *,\n",
    "\t\tC: float = 1.0,\n",
    "\t):\n",
    "\t\tsuper().__init__(\n",
    "\t\t#\tpenalty = None,\n",
    "\t\t#\tdual = False,\n",
    "\t\t#\ttol = 0.0001,\n",
    "\t\t\tC = C, # NOTE: tunable\n",
    "\t\t\tfit_intercept = True,  # use bias\n",
    "\t\t#\tintercept_scaling = 1,\n",
    "\t\t#\tclass_weight = None,\n",
    "\t\t\trandom_state = 42,  # seed\n",
    "\t\t#\tsolver = \"lbfgs\",\n",
    "\t\t#\tmax_iter = 100,\n",
    "\t\t#\tverbose = 0,\n",
    "\t\t#\twarm_start = False,\n",
    "\t\t\tn_jobs = -1,  # parallelization\n",
    "\t\t#\tl1_ratio = None,\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regularization is expeted to be applied (hardcoded) so the strength of regularization is exposed for tuning later. Using the same seed (`42`) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "This is the largest and most important utility that puts everything in one pipeline. Semantically it contains:\n",
    "\n",
    "- a `vectorizer` to do all the text normalization mentioned above\n",
    "- a `model` to do all the machine learning on the features the `vectorizer` got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "\n",
    "\tdef __init__(self, *,\n",
    "\t\tvectorizer: Vectorizer,\n",
    "\t\tmodel: Model,\n",
    "\t):\n",
    "\t\tself.pipeline = sklearn.pipeline.Pipeline(\n",
    "\t\t\t[\n",
    "\t\t\t\t(\"vectorizer\", vectorizer),\n",
    "\t\t\t\t(\"model\", model),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef fit(self, train_data: pandas.DataFrame):\n",
    "\t\tX = train_data.Text\n",
    "\t\ty = train_data.Label\n",
    "\n",
    "\t#\tFit model:\n",
    "\t\tself.pipeline.fit(X, y)\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef predict(self, test_data: pandas.DataFrame,\n",
    "\t\tsave: str | None = None,\n",
    "\t) -> pandas.Series:\n",
    "\t\tX = test_data.Text\n",
    "\t\ty = pandas.Series(self.pipeline.predict(X),\n",
    "\t\t\tindex = test_data.index,  # align with test data index\n",
    "\t\t\tname = \"Label\",  # recover the \"Label\" column name\n",
    "\t\t)\n",
    "\n",
    "\t#\tOptionally save perdictions for submission:\n",
    "\t\tif save is not None:\n",
    "\t\t\ty.to_csv(save)\n",
    "\n",
    "\t\treturn y\n",
    "\n",
    "\tdef evaluate(self, val_data: pandas.DataFrame):\n",
    "\t\ty_true = val_data.Label\n",
    "\t\ty_pred = self.predict(val_data)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"accuracy\"  : sklearn.metrics. accuracy_score(y_true, y_pred),\n",
    "\t\t\t\"presiction\": sklearn.metrics.precision_score(y_true, y_pred),\n",
    "\t\t\t\"recall\"    : sklearn.metrics.   recall_score(y_true, y_pred),\n",
    "\t\t\t\"f1\"        : sklearn.metrics.       f1_score(y_true, y_pred),\n",
    "\t\t}\n",
    "\n",
    "\tdef tune(self,\n",
    "\t\ttrain_data: pandas.DataFrame,\n",
    "\t\tval_data: pandas.DataFrame,\n",
    "\t**param_grid: list):\n",
    "\t\tdata = pandas.concat(\n",
    "\t\t\t[\n",
    "\t\t\t\ttrain_data,\n",
    "\t\t\t\tval_data,\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\n",
    "\t#\tConcatenate train and val split because `sklearn` is a ballbuster for fixed splits:\n",
    "\t\tX = data.Text\n",
    "\t\ty = data.Label\n",
    "\n",
    "\t#\tInitialize a tuner with given parameter grid and splits on the stock model:\n",
    "\t\ttuner = sklearn.model_selection.GridSearchCV(self.pipeline, param_grid,\n",
    "\t\t\tscoring = \"accuracy\",\n",
    "\t\t\tn_jobs = -1,  # parallelization\n",
    "\t\t\trefit = False,  # we will do our own refitting (on the whole data) thank you very much\n",
    "\t\t\tcv = sklearn.model_selection.PredefinedSplit([-1] * len(train_data) + [0] * len(val_data)),  # use our splits\n",
    "\t\t#\tverbose = 0,\n",
    "\t\t#\tpre_dispatch = \"2*n_jobs\",\n",
    "\t\t#\terror_score = numpy.nan,\n",
    "\t\t#\treturn_train_score = False,\n",
    "\t\t)\n",
    "\n",
    "\t#\tFit and tune model:\n",
    "\t\ttuner.fit(X, y)  # type: ignore\n",
    "\n",
    "\t#\tRefit model with the best parameters on the whole data (why throw the val split now that we finished tunning?):\n",
    "\t\tself.pipeline.set_params(**tuner.best_params_)  # set the best parameters for the model\n",
    "\t\tself.pipeline.fit(X, y)  # refit it with the best parameters on the whole dataset available for training\n",
    "\n",
    "\t\treturn self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.fit(train_data: DataFrame)`\n",
    "\n",
    "This method uses the dataset as is to train the pipeline. This means all of the text gets normalized after the vectorizer is fit, then the extracted features are used to fit the model in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.predict(test_data: DataFrame, save = str | None = None)`\n",
    "\n",
    "This method uses the dataset as is to make predictions. The module `sklearn.pipeline` is smart enough to not fit the vectorizer at this (inference) stage, and only normalized text with the previously fit vectorizer. The model then makes predictions on the extracted features. The output is also `pandas`-friendly as a series, always keeping the `ID` indexing to keep proper track of which text is which and with which label.\n",
    "\n",
    "This method is used either in operation or final testing of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `.evaluate(val_data: DataFrame)`\n",
    "\n",
    "This method uses the dataset as is to make evaluate predictions on a known (validation) data split. It compares model predictions on the given data split with the known values in it to assess performance of the pipeline. This was used mostly to peek how the various model settings affected performance.\n",
    "\n",
    "Evaluation on a validation split is also used when tuning the model hyper-parameters, though of course this custom method here is not used. This method was used only for convinience and review during development of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.tune(train_data: DataFrame, val_data: DataFrame, **param_grid: list)`\n",
    "\n",
    "This method wraps creating a grid-search tuner for trying various pipeline settings (as exposed) and runs it on the data given. A little python magic is needed to make `sklearn` work with predifined splits instead of its default $k$-fold splitting, suitable for cross-validation.\n",
    "\n",
    "At the end of the process, the tuner may be able to expose the best model fit during the tuning process. It is preferred however to instantiate the model with the best parameters found instead, so that it gets trained on all the data (both training and validation), since the validation data are no longer useful for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "Below the pipeline is executed on the data given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets load all the dataset splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"train\")\n",
    "val_data   = load_data(\"val\"  )\n",
    "test_data  = load_data(\"test\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets build the classification pipeline and fit on the training data with tuning. The settings chosen to explore are visible in the instantiation of the classifier below, leading to a total of 18 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/home/nikos/Desktop/Learning-Python/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:402: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'arent', 'becaus', 'befor', 'couldnt', 'didnt', 'doe', 'doesnt', 'dont', 'dure', 'ha', 'hadnt', 'hasnt', 'havent', 'hed', 'hell', 'hi', 'id', 'ill', 'im', 'isnt', 'itd', 'itll', 'ive', 'mightnt', 'mustnt', 'neednt', 'onc', 'onli', 'ourselv', 'shant', 'shed', 'shell', 'shouldnt', 'shouldv', 'thatll', 'themselv', 'theyd', 'theyll', 'theyr', 'theyv', 'thi', 'veri', 'wa', 'wasnt', 'wed', 'well', 'werent', 'weve', 'whi', 'wont', 'wouldnt', 'youd', 'youll', 'yourselv', 'youv'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classifier = Classifier(\n",
    "\tvectorizer = Vectorizer(),\n",
    "\tmodel = Model(),\n",
    ")\n",
    "classifier.tune(train_data, val_data,\n",
    "\tvectorizer__max_features = [\n",
    "\t\t128,\n",
    "\t\t256,\n",
    "\t\t512,\n",
    "\t],\n",
    "\tvectorizer__ngram_range = [\n",
    "\t\t(1, 1),\n",
    "\t\t(1, 2),\n",
    "\t\t(2, 2),\n",
    "\t],\n",
    "\tmodel__C = list(numpy.linspace(.5, 1., 2)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets genarate the predictions for submission to the competition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_data,\n",
    "\tsave = \"submission.csv\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
