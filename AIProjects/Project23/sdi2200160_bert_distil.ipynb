{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03210bc",
   "metadata": {},
   "source": [
    "# Project 2.3: Sentiment classification with Distil BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b845f",
   "metadata": {},
   "source": [
    "Standard `python` imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1cf4c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging; logging.basicConfig(level = logging.INFO)\n",
    "import os; os.environ[\"PYTORCHINDUCTOR_LOGLEVEL\"] = \"ERROR\"\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import cast\n",
    "import warnings; warnings.simplefilter(action = \"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43404f",
   "metadata": {},
   "source": [
    "Imported `python` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67156cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.6.0 available.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch; device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4741a",
   "metadata": {},
   "source": [
    "Global functions for book-keeping and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e4695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed: int = 42):\n",
    "\trandom.seed(seed)\n",
    "\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\n",
    "\treturn seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f4276",
   "metadata": {},
   "source": [
    "Helper path constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path.cwd(); root.mkdir(\n",
    "\tparents = True,\n",
    "\texist_ok = True,\n",
    ")\n",
    "models_path = root / \"models\"; models_path.mkdir(\n",
    "\tparents = True,\n",
    "\texist_ok = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776c816",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "A customized HuggingFace dataset dictionary (`datasets.DatasetDict`) to the task for easy configuration for the pipeline. Has a single factory class method for loading all splits of our dataset, with the option to rename columns. The `test` split is (intentionally) missing the labels column.\n",
    "\n",
    "In addition to loading the dataset splits, they are also preprocessed using a Distil BERT tokenizer, and formatted accordingly to turn them into tokenized/pipeline-ready datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f818367",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(datasets.DatasetDict):\n",
    "\n",
    "\t@classmethod\n",
    "\tdef preprocessed(cls,\n",
    "\t\tmodel_name: str = \"bert-base-uncased\",\n",
    "\t\troot: Path = root,\n",
    "\t\ttrim: int | None = None,\n",
    "\t**column_types: datasets.Value):\n",
    "\n",
    "\t\tif not column_types:\n",
    "\t\t\tcolumn_types = dict(\n",
    "\t\t\t\tindex  = datasets.Value(dtype = \"int32\" ),\n",
    "\t\t\t\ttext   = datasets.Value(dtype = \"string\"),\n",
    "\t\t\t\tlabels = datasets.Value(dtype = \"int32\" ),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tfeatures = datasets.Features(column_types)\n",
    "\n",
    "\t\tlogging.info(\"Loading dataset...\")\n",
    "\n",
    "\t\tdataset = cast(datasets.DatasetDict,\n",
    "\t\t\tdatasets.load_dataset(\"csv\",\n",
    "\t\t\t\tname = \"Twitter\",\n",
    "\t\t\t\tdata_files = dict(\n",
    "\t\t\t\t\ttrain = str(root / \"train_dataset.csv\"),\n",
    "\t\t\t\t\tval   = str(root /   \"val_dataset.csv\"),\n",
    "\t\t\t\t\ttest  = str(root /  \"test_dataset.csv\"),\n",
    "\t\t\t\t),\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tlogging.info(\"Dataset loaded.\")\n",
    "\t\tlogging.info(\"Renaming columns...\")\n",
    "\n",
    "\t\tfor split in dataset:\n",
    "\t\t\tcolumns = dict(zip(dataset[split].column_names, column_types))\n",
    "\t\t\tdataset[split] = dataset[split].rename_columns(columns).cast(features)\n",
    "\n",
    "\t\t\tif trim is not None:\n",
    "\t\t\t\tdataset[split] = dataset[split].select(range(trim))\n",
    "\n",
    "\t\tlogging.info(\"Columns renamed.\")\n",
    "\t\tlogging.info(\"Processing dataset...\")\n",
    "\n",
    "\t\ttokenizer = transformers.DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\t\tdef tokenize(batch):\n",
    "\t\t\treturn tokenizer(batch[\"text\"],\n",
    "\t\t\t\tpadding = \"max_length\",\n",
    "\t\t\t\ttruncation = True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdataset = dataset.map(tokenize)\n",
    "\t\tdataset.set_format(\n",
    "\t\t\ttype = \"torch\",\n",
    "\t\t\tcolumns = [\n",
    "\t\t\t\t\"input_ids\",\n",
    "\t\t\t\t\"attention_mask\",\n",
    "\t\t\t\t\"labels\",\n",
    "\t\t\t],\n",
    "\t\t)\n",
    "\t\tdataset[\"test\"] = dataset[\"test\"].remove_columns(\"labels\")\n",
    "\n",
    "\t\tlogging.info(\"Dataset processed.\")\n",
    "\n",
    "\t\treturn cls(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a0df2",
   "metadata": {},
   "source": [
    "## Classification pipeline\n",
    "\n",
    "The entire classification pipeline is wrapped into a classifier class implementing the customary method set:\n",
    "\n",
    "- `__init__`: Define core parameters of the classification pipeline.\n",
    "- `compile`: Initialize all components of the pipeline preparing it for training/evaluation.\n",
    "- `fit`: In effect the so-called training loop.\n",
    "- `evaluate`: The evaluation loop. Only possible if a `val` split is available. Evaluation can never be performed on the `test` split, as it intentionally hides its ground truth (which is necessary for evaluation).\n",
    "- `predict`: Raw methods used for inference from readable text to readable labels.\n",
    "\n",
    "The classification pipeline is augmented to a context manager for using local pretrained models (along with their tokenizer) for it. If a previously saved model is found with the name, it is loaded instead. This is to avoid retraining every time something changes in hos evaluation is done.  Finally, there is a `submit` method for generating the expected `sumbission.csv` from the (unlabelled) `test` split. and a custom `plot` for visualization of the pipeline training and operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClassifier:\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\tmodel_name: str | Path = \"bert-base-uncased\",\n",
    "\t\tnum_labels: int = 2,\n",
    "\t) -> None:\n",
    "\t\tself.trained = (path := models_path / model_name).exists()\n",
    "\n",
    "\t\tself.model_name = model_name if not self.trained else path\n",
    "\t\tself.num_labels = num_labels\n",
    "\n",
    "\t\tself.tokenizer = transformers.DistilBertTokenizer.from_pretrained(model_name)\n",
    "\t\tself.model = transformers.DistilBertForSequenceClassification.from_pretrained(model_name,\n",
    "\t\t\tnum_labels = num_labels,\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef __enter__(self):\n",
    "\t\tlogging.info(f\"Loading model {self.model_name}...\")\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __exit__(self, *_):\n",
    "\t\tself.model.save_pretrained(models_path / self.model_name)\n",
    "\t\tself.tokenizer.save_pretrained(models_path / self.model_name)\n",
    "\n",
    "\t\treturn True\n",
    "\n",
    "\n",
    "\tdef compile(self, dataset: TwitterDataset,\n",
    "\t\ttraining_args: transformers.training_args.TrainingArguments = transformers.training_args.TrainingArguments(\n",
    "\t\t\toutput_dir = \"./results\",\n",
    "\t\t\tlogging_dir = \"./logs\",\n",
    "\n",
    "\t\t\teval_strategy = \"epoch\",\n",
    "\t\t\tsave_strategy = \"epoch\",\n",
    "\n",
    "\t\t\tper_device_train_batch_size = 32,\n",
    "\t\t\tper_device_eval_batch_size = 128,\n",
    "\t\t\tgradient_accumulation_steps = 4,\n",
    "\n",
    "\t\t\tfp16 = True,\n",
    "\n",
    "\t\t\tdataloader_num_workers = 20,\n",
    "\t\t#\tdataloader_persistent_workers = True,\n",
    "\t\t\tdataloader_pin_memory = True,\n",
    "\n",
    "\t\t\tdata_seed = fix_seed(),\n",
    "\t\t\tseed = fix_seed(),\n",
    "\n",
    "\t\t\tnum_train_epochs = 1,\n",
    "\t\t\tlearning_rate = 1e-4,\n",
    "\t\t\tweight_decay = 1e-2,\n",
    "\n",
    "\t\t\tload_best_model_at_end = True,\n",
    "\t\t#\tmetric_for_best_model = \"accuracy\",  # `eval_loss` by default\n",
    "\t\t)\n",
    "\t):\n",
    "\t\tlogging.info(\"Compiling model and initializing its trainer...\")\n",
    "\n",
    "\t\tself.trainer = transformers.trainer.Trainer(\n",
    "\t\t\tmodel = self.model,\n",
    "\t\t\targs = training_args,\n",
    "\t\t\ttrain_dataset = dataset[\"train\"],\n",
    "\t\t\teval_dataset = dataset[\"val\"],\n",
    "\t\t\tprocessing_class = self.tokenizer,\n",
    "\t\t\tcompute_metrics = self.compute_metrics,\n",
    "\t\t)\n",
    "\n",
    "\tdef fit(self) -> dict[str, float]:\n",
    "\t\tif self.trained:\n",
    "\t\t\tlogging.info(\"Model already trained. Skipping training.\")\n",
    "\n",
    "\t\t\treturn dict()\n",
    "\n",
    "\t\tlogging.info(\"Training model...\")\n",
    "\n",
    "\t\tself.model.train()\n",
    "\t\toutput = self.trainer.train()\n",
    "\t\tself.trained = True\n",
    "\n",
    "\t\treturn output.metrics\n",
    "\n",
    "\tdef evaluate(self) -> dict[str, float]:\n",
    "\t\tif not self.trained:\n",
    "\t\t\tlogging.error(\"Model not trained. Cannot evaluate.\")\n",
    "\n",
    "\t\t\treturn dict()\n",
    "\n",
    "\t\tlogging.info(\"Evaluating model...\")\n",
    "\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\treturn self.trainer.evaluate()\n",
    "\n",
    "\n",
    "\tdef predict(self, texts: list[str] | str) -> list[int]:\n",
    "\t\treturn torch.argmax(self.logits(texts),\n",
    "\t\t\tdim = 1,\n",
    "\t\t).tolist()\n",
    "\n",
    "\tdef predict_proba(self, texts: list[str] | str) -> list[float]:\n",
    "\t\treturn torch.softmax(self.logits(texts),\n",
    "\t\t\tdim = 1,\n",
    "\t\t)[:, 1].tolist()\n",
    "\n",
    "\n",
    "\tdef logits(self, texts: list[str] | str) -> torch.Tensor:\n",
    "\t\tif isinstance(texts, str):\n",
    "\t\t\ttexts = [texts]\n",
    "\n",
    "\t\ttokens = self.tokenizer(texts,\n",
    "\t\t\treturn_tensors = \"pt\",\n",
    "\t\t\tpadding = True,\n",
    "\t\t\ttruncation = True,\n",
    "\t\t)\n",
    "\t\ttokens = {k: v.to(device) for k, v in tokens.items()}\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tlogits = self.model(**tokens).logits\n",
    "\n",
    "\t\t\treturn logits.cpu()\n",
    "\n",
    "\t@classmethod\n",
    "\tdef compute_metrics(cls, eval_pred) -> dict[str, float]:\n",
    "\t\tmetrics = evaluate.combine(\n",
    "\t\t\t[\n",
    "\t\t\t\tevaluate.load(\"accuracy\"                     ),\n",
    "\t\t\t\tevaluate.load(\"precision\", average = \"binary\"),\n",
    "\t\t\t\tevaluate.load(\"recall\"   , average = \"binary\"),\n",
    "\t\t\t\tevaluate.load(\"f1\"       , average = \"binary\"),\n",
    "\t\t\t]\n",
    "\t\t)\n",
    "\n",
    "\t\tlogging.info(\"Computing metrics...\")\n",
    "\n",
    "\t\ty_pred, y_true = eval_pred\n",
    "\t\ty_pred = np.argmax(y_pred,\n",
    "\t\t\taxis = 1,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn metrics.compute(\n",
    "\t\t\tpredictions = y_pred,\n",
    "\t\t\treferences  = y_true,\n",
    "\t\t)\n",
    "\n",
    "\tdef plot(self, dataset: TwitterDataset,\n",
    "\t\toutput_dir: Path = Path(\"plots\"),\n",
    "\t):\n",
    "\t\toutput_dir.mkdir(\n",
    "\t\t\tparents = True,\n",
    "\t\t\texist_ok = True,\n",
    "\t\t)\n",
    "\n",
    "\t\tlogging.info(\"Plotting results...\")\n",
    "\n",
    "\t#\tLearning curves:\n",
    "\t\tif self.trainer.state.log_history:\n",
    "\t\t\tlogs = pd.DataFrame(self.trainer.state.log_history)\n",
    "\n",
    "\t\t#\tFilter out unnecessary entries\n",
    "\t\t\ttrain_logs = logs[logs[\"loss\"].notna()]\n",
    "\t\t\teval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "\t\t#\tPlot train vs eval loss:\n",
    "\t\t\tplt.figure()\n",
    "\t\t\tplt.plot(train_logs[\"step\"], train_logs[\"loss\"],\n",
    "\t\t\t\tlabel = \"Train Loss\",\n",
    "\t\t\t)\n",
    "\t\t\tplt.plot(eval_logs[\"step\"], eval_logs[\"eval_loss\"],\n",
    "\t\t\t\tlabel = \"Eval Loss\",\n",
    "\t\t\t)\n",
    "\t\t\tplt.xlabel(\"Step\")\n",
    "\t\t\tplt.ylabel(\"Loss\")\n",
    "\t\t\tplt.legend()\n",
    "\t\t\tplt.title(\"Training vs Evaluation Loss\")\n",
    "\t\t\tplt.savefig(output_dir / \"loss_curve.png\")\n",
    "\t\t\tplt.close()\n",
    "\n",
    "\t\t#\tPlot evaluation metrics:\n",
    "\t\t\tmetrics = [\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\"]\n",
    "\t\t\tfor metric in metrics:\n",
    "\t\t\t\tif metric in eval_logs:\n",
    "\t\t\t\t\tplt.figure()\n",
    "\t\t\t\t\tplt.plot(eval_logs[\"step\"], eval_logs[metric],\n",
    "\t\t\t  \t\t\tlabel = metric,\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tplt.xlabel(\"Step\")\n",
    "\t\t\t\t\tplt.ylabel(metric.split(\"_\")[-1].capitalize())\n",
    "\t\t\t\t\tplt.title(metric.replace(\"_\", \" \").title())\n",
    "\t\t\t\t\tplt.savefig(output_dir / f\"{metric}_curve.png\")\n",
    "\t\t\t\t\tplt.close()\n",
    "\n",
    "\t#\tAUC and Precision-Recall Curve on validation set:\n",
    "\t\tvalidation = self.trainer.predict(dataset[\"val\"])  # type: ignore\n",
    "\t\ty_true = validation.label_ids\n",
    "\t\ty_prob = torch.softmax(torch.tensor(validation.predictions),\n",
    "\t\t\tdim = 1,\n",
    "\t\t)[:, 1].numpy()\n",
    "\t\ty_pred = np.argmax(validation.predictions,\n",
    "\t\t\taxis = 1,\n",
    "\t\t)\n",
    "\n",
    "\t#\tROC Curve:\n",
    "\t\tfpr, tpr, _ = sklearn.metrics.roc_curve(\n",
    "\t\t\ty_true,  # type: ignore\n",
    "\t\t\ty_prob,\n",
    "\t\t)\n",
    "\t\ty_true = validation.label_ids\n",
    "\t\troc_auc = sklearn.metrics.auc(\n",
    "\t\t\tfpr,\n",
    "\t\t\ttpr,\n",
    "\t\t)\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(fpr, tpr,\n",
    "\t\t   \tlabel = f\"ROC AUC = {roc_auc:.2f}\",\n",
    "\t\t)\n",
    "\t\tplt.plot([0, 1], [0, 1],\n",
    "\t\t\tlinestyle = \"--\",\n",
    "\t\t\tcolor = \"gray\",\n",
    "\t\t)\n",
    "\t\tplt.xlabel(\"False Positive Rate\")\n",
    "\t\tplt.ylabel(\"True Positive Rate\")\n",
    "\t\tplt.title(\"ROC Curve\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.savefig(output_dir / \"roc_curve.png\")\n",
    "\t\tplt.close()\n",
    "\n",
    "\t#\tPrecision-Recall Curve:\n",
    "\t\tprecision, recall, _ = sklearn.metrics.precision_recall_curve(\n",
    "\t\t\ty_true,  # type: ignore\n",
    "\t\t\ty_prob,\n",
    "\t\t)\n",
    "\t\ty_true = validation.label_ids\n",
    "\t\tpr_auc = sklearn.metrics.auc(recall, precision)\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(recall, precision,\n",
    "\t\t\tlabel = f\"PR AUC = {pr_auc:.2f}\",\n",
    "\t\t)\n",
    "\t\tplt.xlabel(\"Recall\")\n",
    "\t\tplt.ylabel(\"Precision\")\n",
    "\t\tplt.title(\"Precision-Recall Curve\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.savefig(output_dir / \"pr_curve.png\")\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef submit(self, dataset: TwitterDataset):\n",
    "\t\tlogging.info(\"Submitting predictions...\")\n",
    "\n",
    "\t\tpd.DataFrame(\n",
    "\t\t\tdata = {\n",
    "\t\t\t\t\"index\": dataset[\"test\"][\"index\"],\n",
    "\t\t\t\t\"labels\": self.predict(dataset[\"test\"][\"text\"]),\n",
    "\t\t\t}\n",
    "\t\t).to_csv(\"submission.csv\",\n",
    "\t\t\tindex = False,\n",
    "\t\t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
