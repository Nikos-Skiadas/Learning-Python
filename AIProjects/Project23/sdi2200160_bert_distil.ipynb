{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f03210bc",
   "metadata": {},
   "source": [
    "# Project 2.3: Sentiment classification with Distil BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b845f",
   "metadata": {},
   "source": [
    "Standard `python` imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1cf4c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:52:13.222703Z",
     "iopub.status.busy": "2025-06-05T16:52:13.222446Z",
     "iopub.status.idle": "2025-06-05T16:52:13.230587Z",
     "shell.execute_reply": "2025-06-05T16:52:13.229912Z",
     "shell.execute_reply.started": "2025-06-05T16:52:13.222675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import logging; logging.basicConfig(level = logging.INFO)\n",
    "import os; os.environ[\"PYTORCHINDUCTOR_LOGLEVEL\"] = \"ERROR\"\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import cast\n",
    "import warnings; warnings.simplefilter(action = \"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43404f",
   "metadata": {},
   "source": [
    "Imported `python` libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67156cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:52:13.231481Z",
     "iopub.status.busy": "2025-06-05T16:52:13.231211Z",
     "iopub.status.idle": "2025-06-05T16:52:16.829739Z",
     "shell.execute_reply": "2025-06-05T16:52:16.829153Z",
     "shell.execute_reply.started": "2025-06-05T16:52:13.231455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import torch; device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import datasets\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4741a",
   "metadata": {},
   "source": [
    "Global functions for book-keeping and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8e4695",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:52:16.831689Z",
     "iopub.status.busy": "2025-06-05T16:52:16.831337Z",
     "iopub.status.idle": "2025-06-05T16:52:16.836106Z",
     "shell.execute_reply": "2025-06-05T16:52:16.835299Z",
     "shell.execute_reply.started": "2025-06-05T16:52:16.831669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def fix_seed(seed: int = 42):\n",
    "\trandom.seed(seed)\n",
    "\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\n",
    "\treturn seed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f4276",
   "metadata": {},
   "source": [
    "Helper path constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b85fa41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:52:16.837151Z",
     "iopub.status.busy": "2025-06-05T16:52:16.836899Z",
     "iopub.status.idle": "2025-06-05T16:52:16.849377Z",
     "shell.execute_reply": "2025-06-05T16:52:16.848656Z",
     "shell.execute_reply.started": "2025-06-05T16:52:16.837127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "root = Path.cwd(); root.mkdir(\n",
    "\tparents = True,\n",
    "\texist_ok = True,\n",
    ")\n",
    "models_path = root / \"models\"; models_path.mkdir(\n",
    "\tparents = True,\n",
    "\texist_ok = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776c816",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "A customized HuggingFace dataset dictionary (`datasets.DatasetDict`) to the task for easy configuration for the pipeline. Has a single factory class method for loading all splits of our dataset, with the option to rename columns. The `test` split is (intentionally) missing the labels column.\n",
    "\n",
    "In addition to loading the dataset splits, they are also preprocessed using a Distil BERT tokenizer, and formatted accordingly to turn them into tokenized/pipeline-ready datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f818367",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:52:16.850216Z",
     "iopub.status.busy": "2025-06-05T16:52:16.850040Z",
     "iopub.status.idle": "2025-06-05T16:52:16.860982Z",
     "shell.execute_reply": "2025-06-05T16:52:16.860203Z",
     "shell.execute_reply.started": "2025-06-05T16:52:16.850202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TwitterDataset(datasets.DatasetDict):\n",
    "\n",
    "\t@classmethod\n",
    "\tdef preprocessed(cls,\n",
    "\t\tmodel_name: str = \"distilbert-base-uncased\",\n",
    "\t\troot: Path = root,\n",
    "\t\ttrim: int | None = None,\n",
    "\t**column_types: datasets.Value):\n",
    "\n",
    "\t\tif not column_types:\n",
    "\t\t\tcolumn_types = dict(\n",
    "\t\t\t\tindex  = datasets.Value(dtype = \"int32\" ),\n",
    "\t\t\t\ttext   = datasets.Value(dtype = \"string\"),\n",
    "\t\t\t\tlabels = datasets.Value(dtype = \"int32\" ),\n",
    "\t\t\t)\n",
    "\n",
    "\t\tfeatures = datasets.Features(column_types)\n",
    "\n",
    "\t\tlogging.info(\"Loading dataset...\")\n",
    "\n",
    "\t\tdataset = cast(datasets.DatasetDict,\n",
    "\t\t\tdatasets.load_dataset(\"csv\",\n",
    "\t\t\t\tname = \"Twitter\",\n",
    "\t\t\t\tdata_files = dict(\n",
    "\t\t\t\t\ttrain = str(root / \"train_dataset.csv\"),\n",
    "\t\t\t\t\tval   = str(root /   \"val_dataset.csv\"),\n",
    "\t\t\t\t\ttest  = str(root /  \"test_dataset.csv\"),\n",
    "\t\t\t\t),\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tlogging.info(\"Dataset loaded.\")\n",
    "\t\tlogging.info(\"Renaming columns...\")\n",
    "\n",
    "\t\tfor split in dataset:\n",
    "\t\t\tcolumns = dict(zip(dataset[split].column_names, column_types))\n",
    "\t\t\tdataset[split] = dataset[split].rename_columns(columns).cast(features)\n",
    "\n",
    "\t\t\tif trim is not None:\n",
    "\t\t\t\tdataset[split] = dataset[split].select(range(trim))\n",
    "\n",
    "\t\tlogging.info(\"Columns renamed.\")\n",
    "\t\tlogging.info(\"Processing dataset...\")\n",
    "\n",
    "\t\ttokenizer = transformers.DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\t\tdef tokenize(batch):\n",
    "\t\t\treturn tokenizer(batch[\"text\"],\n",
    "\t\t\t\tpadding = \"max_length\",\n",
    "\t\t\t\ttruncation = True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdataset = dataset.map(tokenize)\n",
    "\t\tdataset.set_format(\n",
    "\t\t\ttype = \"torch\",\n",
    "\t\t\tcolumns = [\n",
    "\t\t\t\t\"input_ids\",\n",
    "\t\t\t\t\"attention_mask\",\n",
    "\t\t\t\t\"labels\",\n",
    "\t\t\t],\n",
    "\t\t)\n",
    "\t\tdataset[\"test\"] = dataset[\"test\"].remove_columns(\"labels\")\n",
    "\n",
    "\t\tlogging.info(\"Dataset processed.\")\n",
    "\n",
    "\t\treturn cls(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a0df2",
   "metadata": {},
   "source": [
    "## Classification pipeline\n",
    "\n",
    "The entire classification pipeline is wrapped into a classifier class implementing the customary method set:\n",
    "\n",
    "- `__init__`: Define core parameters of the classification pipeline.\n",
    "- `compile`: Initialize all components of the pipeline preparing it for training/evaluation.\n",
    "- `fit`: In effect the so-called training loop.\n",
    "- `evaluate`: The evaluation loop. Only possible if a `val` split is available. Evaluation can never be performed on the `test` split, as it intentionally hides its ground truth (which is necessary for evaluation).\n",
    "- `predict`: Raw methods used for inference from readable text to readable labels.\n",
    "\n",
    "The classification pipeline is augmented to a context manager for using local pretrained models (along with their tokenizer) for it. If a previously saved model is found with the name, it is loaded instead. This is to avoid retraining every time something changes in hos evaluation is done.  Finally, there is a `submit` method for generating the expected `sumbission.csv` from the (unlabelled) `test` split. and a custom `plot` for visualization of the pipeline training and operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ae0e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:52:16.862223Z",
     "iopub.status.busy": "2025-06-05T16:52:16.861975Z",
     "iopub.status.idle": "2025-06-05T16:52:22.337472Z",
     "shell.execute_reply": "2025-06-05T16:52:22.336657Z",
     "shell.execute_reply.started": "2025-06-05T16:52:16.862202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 16:52:19.228890: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749142339.251209     131 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749142339.257993     131 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "class TwitterClassifier:\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\tmodel_name: str | Path = \"distilbert-base-uncased\",\n",
    "\t\tnum_labels: int = 2,\n",
    "\t) -> None:\n",
    "\t\tself.trained = (path := models_path / model_name).exists()\n",
    "\n",
    "\t\tself.model_name = model_name if not self.trained else path\n",
    "\t\tself.num_labels = num_labels\n",
    "\n",
    "\t\tself.tokenizer = transformers.DistilBertTokenizer.from_pretrained(model_name)\n",
    "\t\tself.model = transformers.DistilBertForSequenceClassification.from_pretrained(model_name,\n",
    "\t\t\tnum_labels = num_labels,\n",
    "\t\t)\n",
    "\n",
    "\n",
    "\tdef __enter__(self):\n",
    "\t\tlogging.info(f\"Loading model {self.model_name}...\")\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __exit__(self, *_):\n",
    "\t\tself.model.save_pretrained(models_path / self.model_name)\n",
    "\t\tself.tokenizer.save_pretrained(models_path / self.model_name)\n",
    "\n",
    "\t\treturn True\n",
    "\n",
    "\n",
    "\tdef compile(self, dataset: TwitterDataset,\n",
    "\t\ttraining_args: transformers.training_args.TrainingArguments = transformers.training_args.TrainingArguments(\n",
    "\t\t\toutput_dir = \"./results\",\n",
    "\t\t\tlogging_dir = \"./logs\",\n",
    "\n",
    "\t\t\teval_strategy = \"epoch\",\n",
    "\t\t\tsave_strategy = \"epoch\",\n",
    "\n",
    "\t\t\tper_device_train_batch_size = 32,\n",
    "\t\t\tper_device_eval_batch_size = 512,\n",
    "\t\t#\tgradient_accumulation_steps = 4,\n",
    "\n",
    "\t\t\tfp16 = True,\n",
    "\n",
    "\t\t#\tdataloader_num_workers = 20,\n",
    "\t\t#\tdataloader_persistent_workers = True,\n",
    "\t\t\tdataloader_pin_memory = True,\n",
    "\n",
    "\t\t\tdata_seed = fix_seed(),\n",
    "\t\t\tseed = fix_seed(),\n",
    "\n",
    "\t\t\tnum_train_epochs = 1,\n",
    "\t\t\tlearning_rate = 1e-4,\n",
    "\t\t\tweight_decay = 1e-2,\n",
    "\n",
    "\t\t\tload_best_model_at_end = True,\n",
    "\t\t#\tmetric_for_best_model = \"accuracy\",  # `eval_loss` by default\n",
    "\t\t)\n",
    "\t):\n",
    "\t\tlogging.info(\"Compiling model and initializing its trainer...\")\n",
    "\n",
    "\t\tself.trainer = transformers.trainer.Trainer(\n",
    "\t\t\tmodel = self.model,\n",
    "\t\t\targs = training_args,\n",
    "\t\t\ttrain_dataset = dataset[\"train\"],\n",
    "\t\t\teval_dataset = dataset[\"val\"],\n",
    "\t\t\tprocessing_class = self.tokenizer,\n",
    "\t\t\tcompute_metrics = self.compute_metrics,\n",
    "\t\t)\n",
    "\n",
    "\tdef fit(self) -> dict[str, float]:\n",
    "\t\tif self.trained:\n",
    "\t\t\tlogging.info(\"Model already trained. Skipping training.\")\n",
    "\n",
    "\t\t\treturn dict()\n",
    "\n",
    "\t\tlogging.info(\"Training model...\")\n",
    "\n",
    "\t\tself.model.train()\n",
    "\t\toutput = self.trainer.train()\n",
    "\t\tself.trained = True\n",
    "\n",
    "\t\treturn output.metrics\n",
    "\n",
    "\tdef evaluate(self) -> dict[str, float]:\n",
    "\t\tif not self.trained:\n",
    "\t\t\tlogging.error(\"Model not trained. Cannot evaluate.\")\n",
    "\n",
    "\t\t\treturn dict()\n",
    "\n",
    "\t\tlogging.info(\"Evaluating model...\")\n",
    "\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\treturn self.trainer.evaluate()\n",
    "\n",
    "\n",
    "\tdef predict(self, texts: list[str] | str) -> list[int]:\n",
    "\t\treturn torch.argmax(self.logits(texts),\n",
    "\t\t\tdim = 1,\n",
    "\t\t).tolist()\n",
    "\n",
    "\tdef predict_proba(self, texts: list[str] | str) -> list[float]:\n",
    "\t\treturn torch.softmax(self.logits(texts),\n",
    "\t\t\tdim = 1,\n",
    "\t\t)[:, 1].tolist()\n",
    "\n",
    "\tdef logits(self, texts: list[str] | str) -> torch.Tensor:\n",
    "\t\tif isinstance(texts, str):\n",
    "\t\t\ttexts = [texts]\n",
    "\n",
    "\t\tdef tokenize(batch):\n",
    "\t\t\treturn self.tokenizer(batch[\"text\"],\n",
    "\t\t\t\tpadding = \"max_length\",\n",
    "\t\t\t\ttruncation = True,\n",
    "\t\t\t)\n",
    "\n",
    "\t\tdummy_dataset = datasets.Dataset.from_dict({\"text\": texts})\n",
    "\t\tdummy_dataset = dummy_dataset.map(tokenize,\n",
    "\t\t\tbatched = True,\n",
    "\t\t)\n",
    "\n",
    "\t\tdummy_dataset.set_format(\n",
    "\t\t\ttype = \"torch\",\n",
    "\t\t\tcolumns = [\n",
    "\t\t\t\t\"input_ids\",\n",
    "\t\t\t\t\"attention_mask\",\n",
    "\t\t\t],\n",
    "\t\t)\n",
    "\n",
    "\t\treturn torch.tensor(self.trainer.predict(dummy_dataset).predictions)  # type: ignore[arg-type]\n",
    "\n",
    "\n",
    "\t@classmethod\n",
    "\tdef compute_metrics(cls, eval_pred) -> dict[str, float]:\n",
    "\t\tlogging.info(\"Computing metrics...\")\n",
    "\n",
    "\t\ty_pred, y_true = eval_pred\n",
    "\t\ty_pred = np.argmax(y_pred, axis = 1)\n",
    "\n",
    "\t\treturn {\n",
    "\t\t\t\"accuracy\": sklearn.metrics.accuracy_score(y_true, y_pred),\n",
    "\t\t\t\"precision\": sklearn.metrics.precision_score(y_true, y_pred, average = \"binary\"),\n",
    "\t\t\t\"recall\": sklearn.metrics.recall_score(y_true, y_pred, average = \"binary\"),\n",
    "\t\t\t\"f1\": sklearn.metrics.f1_score(y_true, y_pred, average = \"binary\"),\n",
    "\t\t}  # type: ignore[return]\n",
    "\n",
    "\tdef plot(self, dataset: TwitterDataset,\n",
    "\t\toutput_dir: Path = Path(\"plots\"),\n",
    "\t):\n",
    "\t\toutput_dir.mkdir(\n",
    "\t\t\tparents = True,\n",
    "\t\t\texist_ok = True,\n",
    "\t\t)\n",
    "\t\tlogging.info(\"Plotting results...\")\n",
    "\n",
    "\t\t# Learning curves:\n",
    "\t\tif self.trainer.state.log_history:\n",
    "\t\t\tlogs = pd.DataFrame(self.trainer.state.log_history)\n",
    "\n",
    "\t\t\t# Filter out unnecessary entries\n",
    "\t\t\ttrain_logs = logs[logs[\"loss\"].notna()]\n",
    "\t\t\teval_logs = logs[logs[\"eval_loss\"].notna()]\n",
    "\n",
    "\t\t\t# Plot train vs eval loss:\n",
    "\t\t\tplt.figure()\n",
    "\t\t\tplt.plot(train_logs[\"step\"], train_logs[\"loss\"], label=\"Train Loss\")\n",
    "\t\t\tplt.plot(eval_logs[\"step\"], eval_logs[\"eval_loss\"], label=\"Eval Loss\")\n",
    "\t\t\tplt.xlabel(\"Step\")\n",
    "\t\t\tplt.ylabel(\"Loss\")\n",
    "\t\t\tplt.legend()\n",
    "\t\t\tplt.title(\"Training vs Evaluation Loss\")\n",
    "\t\t\tplt.savefig(output_dir / \"loss_curve.png\")\n",
    "\t\t\tplt.close()\n",
    "\n",
    "\t\t\t# Plot evaluation metrics:\n",
    "\t\t\tmetrics = [\"eval_accuracy\", \"eval_precision\", \"eval_recall\", \"eval_f1\"]\n",
    "\t\t\tfor metric in metrics:\n",
    "\t\t\t\tif metric in eval_logs:\n",
    "\t\t\t\t\tplt.figure()\n",
    "\t\t\t\t\tplt.plot(eval_logs[\"step\"], eval_logs[metric], label=metric)\n",
    "\t\t\t\t\tplt.xlabel(\"Step\")\n",
    "\t\t\t\t\tplt.ylabel(metric.split(\"_\")[-1].capitalize())\n",
    "\t\t\t\t\tplt.title(metric.replace(\"_\", \" \").title())\n",
    "\t\t\t\t\tplt.savefig(output_dir / f\"{metric}_curve.png\")\n",
    "\t\t\t\t\tplt.close()\n",
    "\n",
    "\t\t# AUC and Precision-Recall Curve on validation set:\n",
    "\t\tlogging.info(\"Generating ROC and PR curves from classifier predictions...\")\n",
    "\n",
    "\t\ttexts = dataset[\"val\"][\"text\"]\n",
    "\t\ty_true = dataset[\"val\"][\"labels\"]\n",
    "\t\ty_prob = self.predict_proba(texts)\n",
    "\t#\ty_pred = self.predict(texts)\n",
    "\n",
    "\t\t# ROC Curve:\n",
    "\t\tfpr, tpr, _ = sklearn.metrics.roc_curve(y_true, y_prob)\n",
    "\t\troc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(fpr, tpr, label=f\"ROC AUC = {roc_auc:.2f}\")\n",
    "\t\tplt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "\t\tplt.xlabel(\"False Positive Rate\")\n",
    "\t\tplt.ylabel(\"True Positive Rate\")\n",
    "\t\tplt.title(\"ROC Curve\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.savefig(output_dir / \"roc_curve.png\")\n",
    "\t\tplt.close()\n",
    "\n",
    "\t\t# Precision-Recall Curve:\n",
    "\t\tprecision, recall, _ = sklearn.metrics.precision_recall_curve(y_true, y_prob)\n",
    "\t\tpr_auc = sklearn.metrics.auc(recall, precision)\n",
    "\t\tplt.figure()\n",
    "\t\tplt.plot(recall, precision, label=f\"PR AUC = {pr_auc:.2f}\")\n",
    "\t\tplt.xlabel(\"Recall\")\n",
    "\t\tplt.ylabel(\"Precision\")\n",
    "\t\tplt.title(\"Precision-Recall Curve\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.savefig(output_dir / \"pr_curve.png\")\n",
    "\t\tplt.close()\n",
    "\n",
    "\tdef submit(self, dataset: TwitterDataset):\n",
    "\t\tlogging.info(\"Submitting predictions...\")\n",
    "\n",
    "\t\tpd.DataFrame(\n",
    "\t\t\tdata = {\n",
    "\t\t\t\t\"index\": dataset[\"test\"][\"index\"],\n",
    "\t\t\t\t\"labels\": self.predict(dataset[\"test\"][\"text\"]),\n",
    "\t\t\t}\n",
    "\t\t).to_csv(\"submission.csv\",\n",
    "\t\t\tindex = False,\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdfaa9a",
   "metadata": {},
   "source": [
    "## Run\n",
    "\n",
    "Apply the tools setting up the pipeline to run an experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5da335",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-05T16:52:22.338890Z",
     "iopub.status.busy": "2025-06-05T16:52:22.338413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f28cc0d579a48ea86b9f6dd24def5f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3b4630dfa974b8093ef8fffcb41b20e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2276081cfe242d497b2bdb99cd594bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851b2b209d164f36b97c4868a48cab98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/148388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d097fbf016cd4d73ad55afe4f87a748c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/42396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea8b14a4a99a456db3566da4d26686be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/21199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a54836aa6c1438a9afc0a75fdbde94f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b19146c93b47cba3bba76fceab3eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d577b2b0e8143c1a69b080ef4f1a7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aea1cd02ba64b81b0faffed38c7e784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6318a5609f482cab9ce8e259477506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/148388 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0827d6c886f4ac2a8165592f9d46f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/42396 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755dd9e04fac4a808880c7cb5087142f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21199 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n",
      "You are using a model of type bert to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fcffad64094692be45a8e6a2c2b7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'distilbert.embeddings.LayerNorm.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.10.attention.k_lin.bias', 'distilbert.transformer.layer.10.attention.k_lin.weight', 'distilbert.transformer.layer.10.attention.out_lin.bias', 'distilbert.transformer.layer.10.attention.out_lin.weight', 'distilbert.transformer.layer.10.attention.q_lin.bias', 'distilbert.transformer.layer.10.attention.q_lin.weight', 'distilbert.transformer.layer.10.attention.v_lin.bias', 'distilbert.transformer.layer.10.attention.v_lin.weight', 'distilbert.transformer.layer.10.ffn.lin1.bias', 'distilbert.transformer.layer.10.ffn.lin1.weight', 'distilbert.transformer.layer.10.ffn.lin2.bias', 'distilbert.transformer.layer.10.ffn.lin2.weight', 'distilbert.transformer.layer.10.output_layer_norm.bias', 'distilbert.transformer.layer.10.output_layer_norm.weight', 'distilbert.transformer.layer.10.sa_layer_norm.bias', 'distilbert.transformer.layer.10.sa_layer_norm.weight', 'distilbert.transformer.layer.11.attention.k_lin.bias', 'distilbert.transformer.layer.11.attention.k_lin.weight', 'distilbert.transformer.layer.11.attention.out_lin.bias', 'distilbert.transformer.layer.11.attention.out_lin.weight', 'distilbert.transformer.layer.11.attention.q_lin.bias', 'distilbert.transformer.layer.11.attention.q_lin.weight', 'distilbert.transformer.layer.11.attention.v_lin.bias', 'distilbert.transformer.layer.11.attention.v_lin.weight', 'distilbert.transformer.layer.11.ffn.lin1.bias', 'distilbert.transformer.layer.11.ffn.lin1.weight', 'distilbert.transformer.layer.11.ffn.lin2.bias', 'distilbert.transformer.layer.11.ffn.lin2.weight', 'distilbert.transformer.layer.11.output_layer_norm.bias', 'distilbert.transformer.layer.11.output_layer_norm.weight', 'distilbert.transformer.layer.11.sa_layer_norm.bias', 'distilbert.transformer.layer.11.sa_layer_norm.weight', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.6.attention.k_lin.bias', 'distilbert.transformer.layer.6.attention.k_lin.weight', 'distilbert.transformer.layer.6.attention.out_lin.bias', 'distilbert.transformer.layer.6.attention.out_lin.weight', 'distilbert.transformer.layer.6.attention.q_lin.bias', 'distilbert.transformer.layer.6.attention.q_lin.weight', 'distilbert.transformer.layer.6.attention.v_lin.bias', 'distilbert.transformer.layer.6.attention.v_lin.weight', 'distilbert.transformer.layer.6.ffn.lin1.bias', 'distilbert.transformer.layer.6.ffn.lin1.weight', 'distilbert.transformer.layer.6.ffn.lin2.bias', 'distilbert.transformer.layer.6.ffn.lin2.weight', 'distilbert.transformer.layer.6.output_layer_norm.bias', 'distilbert.transformer.layer.6.output_layer_norm.weight', 'distilbert.transformer.layer.6.sa_layer_norm.bias', 'distilbert.transformer.layer.6.sa_layer_norm.weight', 'distilbert.transformer.layer.7.attention.k_lin.bias', 'distilbert.transformer.layer.7.attention.k_lin.weight', 'distilbert.transformer.layer.7.attention.out_lin.bias', 'distilbert.transformer.layer.7.attention.out_lin.weight', 'distilbert.transformer.layer.7.attention.q_lin.bias', 'distilbert.transformer.layer.7.attention.q_lin.weight', 'distilbert.transformer.layer.7.attention.v_lin.bias', 'distilbert.transformer.layer.7.attention.v_lin.weight', 'distilbert.transformer.layer.7.ffn.lin1.bias', 'distilbert.transformer.layer.7.ffn.lin1.weight', 'distilbert.transformer.layer.7.ffn.lin2.bias', 'distilbert.transformer.layer.7.ffn.lin2.weight', 'distilbert.transformer.layer.7.output_layer_norm.bias', 'distilbert.transformer.layer.7.output_layer_norm.weight', 'distilbert.transformer.layer.7.sa_layer_norm.bias', 'distilbert.transformer.layer.7.sa_layer_norm.weight', 'distilbert.transformer.layer.8.attention.k_lin.bias', 'distilbert.transformer.layer.8.attention.k_lin.weight', 'distilbert.transformer.layer.8.attention.out_lin.bias', 'distilbert.transformer.layer.8.attention.out_lin.weight', 'distilbert.transformer.layer.8.attention.q_lin.bias', 'distilbert.transformer.layer.8.attention.q_lin.weight', 'distilbert.transformer.layer.8.attention.v_lin.bias', 'distilbert.transformer.layer.8.attention.v_lin.weight', 'distilbert.transformer.layer.8.ffn.lin1.bias', 'distilbert.transformer.layer.8.ffn.lin1.weight', 'distilbert.transformer.layer.8.ffn.lin2.bias', 'distilbert.transformer.layer.8.ffn.lin2.weight', 'distilbert.transformer.layer.8.output_layer_norm.bias', 'distilbert.transformer.layer.8.output_layer_norm.weight', 'distilbert.transformer.layer.8.sa_layer_norm.bias', 'distilbert.transformer.layer.8.sa_layer_norm.weight', 'distilbert.transformer.layer.9.attention.k_lin.bias', 'distilbert.transformer.layer.9.attention.k_lin.weight', 'distilbert.transformer.layer.9.attention.out_lin.bias', 'distilbert.transformer.layer.9.attention.out_lin.weight', 'distilbert.transformer.layer.9.attention.q_lin.bias', 'distilbert.transformer.layer.9.attention.q_lin.weight', 'distilbert.transformer.layer.9.attention.v_lin.bias', 'distilbert.transformer.layer.9.attention.v_lin.weight', 'distilbert.transformer.layer.9.ffn.lin1.bias', 'distilbert.transformer.layer.9.ffn.lin1.weight', 'distilbert.transformer.layer.9.ffn.lin2.bias', 'distilbert.transformer.layer.9.ffn.lin2.weight', 'distilbert.transformer.layer.9.output_layer_norm.bias', 'distilbert.transformer.layer.9.output_layer_norm.weight', 'distilbert.transformer.layer.9.sa_layer_norm.bias', 'distilbert.transformer.layer.9.sa_layer_norm.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fix_seed()\n",
    "\n",
    "dataset = TwitterDataset.preprocessed(\n",
    "    root = Path(\"/kaggle/input/ai-2-dl-for-nlp-2025-homework-3\")\n",
    ")\n",
    "\n",
    "with TwitterClassifier(\"distilbert-base-uncased\") as classifier:\n",
    "\tclassifier.compile(dataset)\n",
    "\tclassifier.fit()\n",
    "\tclassifier.evaluate()\n",
    "\tclassifier.plot(dataset)\n",
    "\tclassifier.submit(dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11781620,
     "sourceId": 98723,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
