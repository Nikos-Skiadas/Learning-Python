{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8465e5cb",
   "metadata": {},
   "source": [
    "# Project 2.2: Sentiment classification with MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a306c5",
   "metadata": {},
   "source": [
    "System imports to skip silly warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8a2ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.environ[\"PYTORCHINDUCTOR_LOGLEVEL\"] = \"ERROR\"\n",
    "import warnings; warnings.simplefilter(action = \"ignore\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b618b7",
   "metadata": {},
   "source": [
    "Standard `python` imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5036c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections import Counter\n",
    "from functools import wraps\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, Iterable, Literal, Self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1079c2",
   "metadata": {},
   "source": [
    "Useful only when running notebook locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92f9b680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "from rich.progress import Progress, track"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7869b0",
   "metadata": {},
   "source": [
    "Data and numerical libraries. Note that we set `torch` to use `cuda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d421eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import sklearn.utils\n",
    "import torch; torch.set_default_device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c267a1",
   "metadata": {},
   "source": [
    "`nltk` specific import and initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2858dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/terraformer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/terraformer/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import nltk.stem\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba642c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o /usr/share/nltk_data/corpora/wordnet.zip   -d /usr/share/nltk_data/corpora/\n",
    "!unzip -o /usr/share/nltk_data/corpora/stopwords.zip -d /usr/share/nltk_data/corpora/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e4ccb",
   "metadata": {},
   "source": [
    "Create a `cache` folder for caching results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af4a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_path = Path.cwd() / \"cache\"; cache_path.parent.mkdir(\n",
    "\tparents = True,\n",
    "\texist_ok = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b4a9fd",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "\n",
    "Some utility functions that are needed here and there and did not bother encapsulating object-oriented style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aabc5e",
   "metadata": {},
   "source": [
    "Fix seed across numerical libraries for reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da0899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed: int = 42):\n",
    "\trandom.seed(seed)\n",
    "\n",
    "\tnp.random.seed(seed)\n",
    "\tsklearn.utils.check_random_state(seed)\n",
    "\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)  # if using multi-GPU\n",
    "\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c532c7",
   "metadata": {},
   "source": [
    "Decorator to make whichever method returning a serializable output cachable. It is a quick and dirty cache for a single pass for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130485f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preload(method):\n",
    "\tcache_path = Path.cwd() / \"cache\"; cache_path.parent.mkdir(parents = True, exist_ok = True)\n",
    "\tcache_file = cache_path / method.__name__; cache_file = cache_file.with_suffix(\".pt\")\n",
    "\n",
    "\t@wraps(method)\n",
    "\tdef wrapper(self, *args, **kwargs):\n",
    "\t\tif cache_file.is_file():\n",
    "\t\t\twith cache_file.open(\"rb\") as f:\n",
    "\t\t\t\treturn torch.load(f)\n",
    "\n",
    "\t\tresult = method(self, *args, **kwargs)\n",
    "\n",
    "\t\twith cache_file.open(\"w+b\") as f:\n",
    "\t\t\ttorch.save(result, f)\n",
    "\n",
    "\t\treturn result\n",
    "\n",
    "\treturn wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c9628b",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Utilities focused on preprocessing text in the data according to the task given (sentiment classification on messages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31561e0c",
   "metadata": {},
   "source": [
    "We have Twitter messages so remove:\n",
    "\n",
    "- `@` mentions\n",
    "- `#` hashtags\n",
    "- emails\n",
    "- punctuation\n",
    "\n",
    "We also lower the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "\tdef __call__(self, text: str) -> str:\n",
    "\t\ttext = re.sub(r\"@\\w+\"   , \"\", text)  # remove mentions\n",
    "\t\ttext = re.sub(r\"#\\w+\"   , \"\", text)  # remove hashtags\n",
    "\t\ttext = re.sub(r\"\\S+@\\S+\", \"\", text)  # remove emails\n",
    "\n",
    "\t\treturn text.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887aef85",
   "metadata": {},
   "source": [
    "Tokenize text. This is were we lower the case. It happens in two places for berbosity and safety. Tokenization accounts for english stopwords as messages are in english."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "\n",
    "\tdef __init__(self):\n",
    "\t\tself.stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\t\tself.lemmatizer = nltk.WordNetLemmatizer()\n",
    "\t\tself.stemmer = nltk.stem.PorterStemmer()\n",
    "\t\tself.tokenizer = nltk.tokenize.TweetTokenizer(\n",
    "\t\t\tpreserve_case = False,  # ignore case (lower it)\n",
    "\t\t\treduce_len = True,  # reduce repeated characters (e.g. \"sooo\" -> \"so\")\n",
    "\t\t\tstrip_handles = True,  # remove @mentions also happening in preprocessing\n",
    "\t\t)\n",
    "\n",
    "\tdef __call__(self, text: str):\n",
    "\t\ttokens = []\n",
    "\n",
    "\t\tfor token in self.tokenizer.tokenize(text):\n",
    "\t\t\ttoken = token.lower()\n",
    "\n",
    "\t\t\tif token and not token.isdigit() and token not in self.stopwords:\n",
    "\t\t\t\ttoken = self.lemmatizer.lemmatize(token)\n",
    "\t\t\t\ttoken = self.stemmer.stem(token)\n",
    "\n",
    "\t\t\t\ttokens.append(token)\n",
    "\n",
    "\t\treturn tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13da26d0",
   "metadata": {},
   "source": [
    "Both are callable so we can stack them in one preprocessing callable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05922fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(text: str) -> list[str]:\n",
    "\ttext = Preprocessor()(text)\n",
    "\ttokens = Tokenizer()(text)\n",
    "\n",
    "\treturn tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c6a56",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "What follows are utilities that help us import and process incoming data for the actual computation layers of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9b5d4a",
   "metadata": {},
   "source": [
    "The following is a word index augmented with padding and unknown tokens at `0` and `1` respectively. Calling it on an iterable of tokens will return a sequence of their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e138a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(dict[str, int]):\n",
    "\n",
    "\tdef __init__(self, word2idx: dict[str, int], *,\n",
    "\t\tpad_token = \"<pad>\",\n",
    "\t\tunk_token = \"<unk>\",\n",
    "\t):\n",
    "\t\tsuper().__init__(word2idx)\n",
    "\n",
    "\t\tself.pad_token = pad_token\n",
    "\t\tself.unk_token = unk_token\n",
    "\n",
    "\t\tself.pad_idx = self.get(pad_token, 0)\n",
    "\t\tself.unk_idx = self.get(unk_token, 1)\n",
    "\n",
    "\tdef __call__(self, tokens: list[str]) -> list[int]:\n",
    "\t\treturn [self.get(token, self.unk_idx) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52565df9",
   "metadata": {},
   "source": [
    "The following is a callable to translate incoming text into token indices to be fed to an embedding layer (to follow). It uses a vocabulary (word index) given (regardless where it came from or how it was built) to procude the indices. It applies the preprocessing and tokenization to the text. Lastly it trims or pads the output index sequence by a fixed length if given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d857e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTransform:\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\tvocabulary: Vocabulary,\n",
    "\t\tpreprocessor: Callable | None = None,\n",
    "\t\ttokenizer: Callable | None = None,\n",
    "\t\tmax_len: int | None = None,\n",
    "\t):\n",
    "\t\tself.vocabulary = vocabulary\n",
    "\t\tself.preprocessor = preprocessor\n",
    "\t\tself.tokenizer = tokenizer\n",
    "\t\tself.max_len = max_len\n",
    "\n",
    "\tdef __call__(self, text: str) -> torch.Tensor:\n",
    "\t\tif self.preprocessor: text = self.preprocessor(text)\n",
    "\t\tif self.tokenizer: tokens = self.tokenizer(text)\n",
    "\t\telse: tokens = text.split()\n",
    "\n",
    "\t\tindices = self.vocabulary(tokens)\n",
    "\n",
    "\t\tif self.max_len is not None:\n",
    "\t\t\tif len(indices) < self.max_len: indices += [self.vocabulary.pad_idx] * (self.max_len - len(indices))\n",
    "\t\t\telse: indices = indices[:self.max_len]\n",
    "\n",
    "\t\treturn torch.tensor(indices,\n",
    "\t\t\tdtype = torch.long,\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540190a",
   "metadata": {},
   "source": [
    "This is a custom dataset object tailored to work with `torch` data loaders. We instantiate it based on desired split for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(torch.utils.data.Dataset):\n",
    "\n",
    "\tdef __init__(self, split: Literal[\"train\", \"val\", \"test\"], *,\n",
    "\t\ttransform: Callable,\n",
    "\t):\n",
    "\t\tself.data = self.load_data(split).reset_index()\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "\tdef __getitem__(self, idx: int | slice):\n",
    "\t\treturn self.transform(self.data.Text[idx]), torch.tensor(self.data.Label[idx],\n",
    "\t\t\tdtype = torch.float,\n",
    "\t\t)  # return tensor of token indices and label\n",
    "\n",
    "\n",
    "\t@classmethod\n",
    "\tdef load_data(cls, split: Literal[\n",
    "\t\t\t\"train\",\n",
    "\t\t\t\"val\",\n",
    "\t\t\t\"test\",\n",
    "\t\t],\n",
    "\t\troot: str = \"/kaggle/input/ai-2-dl-for-nlp-2025-homework-2\",\n",
    "\t\tindex: str = \"ID\",\n",
    "\t):\n",
    "\t\treturn pd.read_csv(Path(root) / f\"{split}_dataset.csv\",\n",
    "\t\t\tindex_col = index,  # Set ID column as index\n",
    "\t\t\tencoding = \"utf-8\",\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02812b5",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "The various steps from messages to sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5523bab",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "The following is a custom embedding layer made to:\n",
    "\n",
    "- load `glove` embeddings (of our chosen dimension) into `word2vec` format\n",
    "- extract a vocabulary (word index) and a sequence of vectors from the `word2vec` we made\n",
    "- create a pretrained embedding layer for our pipeline with the weights matching text based on the word index and vector sequence we got from the `word2vec` we made\n",
    "\n",
    "Note that we statically store the word index in case we later edit it (for example sort it based on term frequency in a corpus). This is a dangerous practice and maybe can be done in a better way, but eventually we do not use `prune_with_frequencies` so we are not worried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231dd1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(torch.nn.Embedding):\n",
    "\n",
    "\tword2idx: Vocabulary  # word to index mapping\n",
    "\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_glove(cls,\n",
    "\t\tembedding_dim: Literal[50, 100, 200, 300] = 50,\n",
    "\t\tfreeze: bool = False,\n",
    "\t\tpad_token: str = \"<pad>\",\n",
    "\t\tunk_token: str = \"<unk>\",\n",
    "\t**kwargs) -> Self:\n",
    "\t\tsource_path: Path = Path(\"embeddings\") / f\"glove.6B.{embedding_dim}d.txt\"\n",
    "\t\ttarget_path: Path = source_path.with_suffix(\".word2vec.txt\")\n",
    "\n",
    "\t\tcls.convert_glove_to_word2vec(source_path, target_path)\n",
    "\n",
    "\t\tword2idx, tensor = cls.load_word2vec_format(target_path)\n",
    "\n",
    "\t\t# Insert special tokens:\n",
    "\t\tpad_vector = torch.zeros(tensor.shape[1], device = tensor.device)\n",
    "\t\tunk_vector = torch.randn(tensor.shape[1], device = tensor.device) * 0.1  # smaller variance\n",
    "\n",
    "\t\t# Rebuild mapping with special tokens:\n",
    "\t\tword2idx = {\n",
    "\t\t\tpad_token: 0,\n",
    "\t\t\tunk_token: 1,\n",
    "\t\t**{word: idx + 2 for word, idx in word2idx.items()}}\n",
    "\n",
    "\t\t# Stack special vectors:\n",
    "\t\ttensor = torch.vstack(\n",
    "\t\t\t[\n",
    "\t\t\t\tpad_vector,\n",
    "\t\t\t\tunk_vector,\n",
    "\t\t\ttensor]\n",
    "\t\t)\n",
    "\n",
    "\t\tself = cls.from_pretrained(tensor,\n",
    "\t\t\tfreeze = freeze,\n",
    "\t\t**kwargs)\n",
    "\t\tself.word2idx = Vocabulary(word2idx,\n",
    "\t\t\tpad_token = pad_token,\n",
    "\t\t\tunk_token = unk_token,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\t@classmethod\n",
    "\tdef convert_glove_to_word2vec(cls,\n",
    "\t\tsource_path: Path,\n",
    "\t\ttarget_path: Path | None = None,\n",
    "\t):\n",
    "\t\twith open(source_path, \"r+\", encoding=\"utf-8\") as source_file:\n",
    "\t\t\tlines = source_file.readlines()  # read all lines\n",
    "\n",
    "\t\tnum_tokens, embedding_dim = len(lines), len(lines[0].strip().split()) - 1  # count vocabulary size and embeddings dimension\n",
    "\n",
    "\t\twith open(target_path if target_path is not None else source_path, \"w+\", encoding=\"utf-8\") as target_file:\n",
    "\t\t\ttarget_file.write(f\"{num_tokens} {embedding_dim}\\n\")  # write the `word2vec` header\n",
    "\n",
    "\t\t\tfor line in track(lines, \"converting glove to word2vec\".ljust(32), num_tokens):\n",
    "\t\t\t\ttarget_file.write(line)  # copy the rest of the lines\n",
    "\n",
    "\t@classmethod\n",
    "\tdef load_word2vec_format(cls, word2vec_path: Path) -> tuple[Vocabulary, torch.Tensor]:\n",
    "\t\twith open(word2vec_path, \"r+\", encoding=\"utf-8\") as word2vec_file:\n",
    "\t\t\tnum_tokens, embedding_dim = map(int, word2vec_file.readline().strip().split())\n",
    "\n",
    "\t\t\tword2idx = {}  # word to index mapping\n",
    "\t\t\tvectors = torch.zeros(num_tokens, embedding_dim)  # preallocate tensor for word vectors\n",
    "\n",
    "\t\t\tfor index, line in track(enumerate(word2vec_file), \"get embeddings from word2vec\".ljust(32), num_tokens):\n",
    "\t\t\t\tword, *vec = line.strip().split()  # split word and vector\n",
    "\t\t\t\tvec = torch.tensor(list(map(float, vec)))  # convert vector to tensor\n",
    "\n",
    "\t\t\t\tword2idx[word] = index  # map word to index\n",
    "\t\t\t\tvectors[index] = vec  # assign vector to the corresponding index\n",
    "\n",
    "\t\treturn Vocabulary(word2idx), vectors\n",
    "\n",
    "\n",
    "\tdef index(self, key: str | Iterable[str]) -> int | list[int]:\n",
    "\t\treturn [self.word2idx[item] for item in key] if isinstance(key, Iterable) else self.word2idx[key]\n",
    "\n",
    "\tdef get(self, key: str | Iterable[str]) -> torch.Tensor:\n",
    "\t\treturn self.weight[self.index(key)]\n",
    "\n",
    "\tdef prune_with_frequencies(self,\n",
    "\t\tfrequencies: Counter[str],\n",
    "\t\tmin_frequency: int = 1,\n",
    "\t\tmax_vocab_size: int | None = None,\n",
    "\t\tpad_token: str = \"<pad>\",\n",
    "\t\tunk_token: str = \"<unk>\",\n",
    "\t) -> Self:\n",
    "\t\ttokens = [token for token, frequency in frequencies.items() if frequency >= min_frequency and token in self.word2idx]\n",
    "\n",
    "\t# \tSort tokens by frequency (desc), then alphabetically (asc):\n",
    "\t\ttokens.sort(\n",
    "\t\t\tkey = lambda token: (-frequencies[token], token)\n",
    "\t\t)\n",
    "\n",
    "\t#\tApply vocab size cap:\n",
    "\t\ttokens = tokens[:max_vocab_size]\n",
    "\n",
    "\t#\tBuild pruned token list:\n",
    "\t\tall_tokens = [\n",
    "\t\t\tpad_token,\n",
    "\t\t\tunk_token,\n",
    "\t\t] + tokens\n",
    "\n",
    "\t\tword2idx = {token: idx for idx, token in enumerate(all_tokens)}\n",
    "\n",
    "\t#\tBuild new weight matrix:\n",
    "\t\tembedding_dim = self.embedding_dim\n",
    "\t\tnew_weights = torch.zeros(len(all_tokens), embedding_dim)\n",
    "\n",
    "\t\tfor token, new_idx in word2idx.items():\n",
    "\t\t\tif   token == pad_token: continue\n",
    "\t\t\telif token == unk_token: new_weights[new_idx] = torch.randn(embedding_dim) * 0.1\n",
    "\t\t\telse:\n",
    "\t\t\t\told_idx = self.word2idx[token]\n",
    "\t\t\t\tnew_weights[new_idx] = self.weight[old_idx]\n",
    "\n",
    "\t#\tCreate new embedding layer with new weights:\n",
    "\t\tnew_embedding = self.from_pretrained(new_weights)\n",
    "\t\tnew_embedding.word2idx = Vocabulary(word2idx,\n",
    "\t\t\tpad_token = pad_token,\n",
    "\t\t\tunk_token = unk_token,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn new_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a6e45",
   "metadata": {},
   "source": [
    "### Compount (hidden) layer\n",
    "\n",
    "In creating a basic deep feed-forward network, it is customary to:\n",
    "\n",
    "- apply activation on each layer otherwise it is not deep learning we are doing (deep linear networks reduce to flat linear regression)\n",
    "- apply dropout for statistical regularization (create random variances of the model during training instead of explicitely training various models)\n",
    "\n",
    "Notice the choices we make about activation:\n",
    "\n",
    "- We decided to use `SiLU`, which is a smooth variant of `ReLU` with negative slope near the switch point, shown to perform slightly better.\n",
    "- We apply it as preactivation, meaning activation precedes the layer. This makes no difference for the hidden layers, but only for the input and output layers. That way we apply activation on the input embeddings (it is not necessary, but it is not harmful either) and the output layer is treated as a logit anyways, so we do not mind if the output is not non-negative (strictly speaking even with `SiLU` it would be slightly negative).\n",
    "\n",
    "Dropout is also applied before, therefore we randomly dropout input features too, which is desireable to avoid memorizing input data and create small data variances during training.\n",
    "\n",
    "This combound layer is intended to be repeated as many times as we like as a hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98251651",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterLayer(torch.nn.Sequential):\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\tinputs_dim: int,\n",
    "\t\toutput_dim: int | None = None,\n",
    "\t\tdropout: float = 0.5,\n",
    "\t):\n",
    "\t\tsuper().__init__(\n",
    "\t\t\ttorch.nn.SiLU(),\n",
    "\t\t\ttorch.nn.Dropout(\n",
    "\t\t\t\tp = dropout,\n",
    "\t\t\t),\n",
    "\t\t\ttorch.nn.Linear(inputs_dim, output_dim or inputs_dim),\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b0bf1",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "This is the model we use for the task. It consists of:\n",
    "\n",
    "- an embedding layer\n",
    "- an input layer mapping to a fixed `hidden_dim` dimension\n",
    "- several (up to a given number) hidden layers of fixed `hidden_dim` dimension\n",
    "- an output layer with one dimension which is enough for a `0` or (to) `1` classification (or probability/logistic regression)\n",
    "\n",
    "Notice that the embedding layer uses average pooling of the word embeddings it extracts to form a sentence embedding for the enicoming message, before feeding it to the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterModel(torch.nn.Module):\n",
    "\n",
    "\tdef __init__(self, embedding: Embedding,\n",
    "\t\thidden_dim: int | list[int] = 100,\n",
    "\t\tnum_layers: int = 2,  # ignored if hidden_dim is a list\n",
    "\t\tdropout: float = 0.5,\n",
    "\t):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.embedding = embedding\n",
    "\n",
    "\t\tself.input_dim = self.embedding.embedding_dim\n",
    "\t\tself.output_dim = 1  # binary classification (positive/negative)\n",
    "\n",
    "\t\tif isinstance(hidden_dim, int):\n",
    "\t\t\thidden_dim = [hidden_dim] * num_layers\n",
    "\n",
    "\t\tlayer_dims = [self.input_dim] + hidden_dim + [self.output_dim]  # input and output dimensions along with hidden dimensions\n",
    "\n",
    "\t\tself.model = torch.nn.Sequential(\n",
    "\t\t\t*(\n",
    "\t\t\t\tTwitterLayer(\n",
    "\t\t\t\t\tinputs_dim = inputs_dim,\n",
    "\t\t\t\t\toutput_dim = output_dim, dropout = dropout\n",
    "\t\t\t\t) for inputs_dim, output_dim in zip(\n",
    "\t\t\t\t\tlayer_dims[ :-1],\n",
    "\t\t\t\t\tlayer_dims[1:  ],\n",
    "\t\t\t\t)\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tprint()\n",
    "\t\tprint(f\"Model summary:\")\n",
    "\t\tprint()\n",
    "\t\tprint(self)\n",
    "\t\tprint()\n",
    "\n",
    "\tdef forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "\t\tembeddings = self.embedding.forward(input)\n",
    "\t\tmask = (input != self.embedding.word2idx.pad_idx).unsqueeze(-1).float()\n",
    "\t\tembeddings *= mask\n",
    "\t\tpooled = embeddings.sum(1) / mask.sum(1).clamp(\n",
    "\t\t\tmin = 1e-6,\n",
    "\t\t)\n",
    "\n",
    "\t\treturn self.model.forward(pooled).squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f212dd4",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "All this is put into one classifier pipeline which contains training and evaluation loops plus other utilities:\n",
    "\n",
    "- `compile` assigns an optimizer and a loss for training for the pipeline with sensible defaults\n",
    "- `prune` is helper function to trim the word index of the embedding layer according to word frequency used in the dataset fed to it\n",
    "- `fit` is the training loop given a dataset (supports `torch` data loaded keyword arguments on top)\n",
    "- `evaluate` is the test loop given a dataset\n",
    "- `predict` is translating model output to readable output (crip sentiment decision `0` or `1` instead of a probability) using threhsold `.5`\n",
    "- `compute` is a generic computation loop to avoid repeatition of code in the `fit` and `evaluate` methods\n",
    "- `submit` will generate the requested `submission.csv` file\n",
    "\n",
    "There are several other methods focused on producing meaningful reports and plots upon evaluuation.\n",
    "\n",
    "Also note that the classifier is a context manager too. If used that way, it will try and load a previous cached trained model state to continue from or evaluate on.\n",
    "\n",
    "Finally, the `preload` decorator is applied on all methods that return metrics so that they are not recomputed. This will be removed soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClassifier:\n",
    "\n",
    "\tdef __init__(self, model: TwitterModel,\n",
    "\t\tmax_len: int = 32,\n",
    "\t\tpath: Path = cache_path / \"model.pt\",\n",
    "\t):\n",
    "\t\tself.model = model\n",
    "\t\tself.max_len = max_len\n",
    "\t\tself.path = path\n",
    "\n",
    "\tdef __enter__(self) -> Self:\n",
    "\t\tif self.path is not None and self.path.is_file():\n",
    "\t\t\twith self.path.open(\"rb+\") as file:\n",
    "\t\t\t\tself.model.load_state_dict(torch.load(file))\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef __exit__(self, *_):\n",
    "\t\tif self.path is not None:\n",
    "\t\t\twith self.path.open(\"wb+\") as file:\n",
    "\t\t\t\ttorch.save(self.model.state_dict(), file)\n",
    "\n",
    "\n",
    "\tdef compile(self,\n",
    "\t\tlearning_rate: float = 1e-3,\n",
    "\t\tweight_decay : float = 0,\n",
    "\t):\n",
    "\t\tdef accuracy(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\treturn (y_pred * y_true).mean()\n",
    "\n",
    "\t\tdef precision(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\ttrue_positive = (y_pred * y_true).sum()\n",
    "\t\t\tpredicted_positive = y_pred.sum()\n",
    "\n",
    "\t\t\treturn true_positive / predicted_positive.clamp(1e-6)\n",
    "\n",
    "\t\tdef recall(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\ttrue_positive = (y_pred * y_true).sum()\n",
    "\t\t\tactual_positive = y_true.sum()\n",
    "\n",
    "\t\t\treturn true_positive / actual_positive.clamp(1e-6)\n",
    "\n",
    "\t\tdef f1(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
    "\t\t\tp = precision(y_pred, y_true)\n",
    "\t\t\tr = recall(y_pred, y_true)\n",
    "\n",
    "\t\t\treturn 2 * p * r / (p + r).clamp(1e-6)\n",
    "\n",
    "\t\tself.model.to(device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "\t\tself.optimizer = torch.optim.AdamW(self.model.parameters(),\n",
    "\t\t\tlr = learning_rate,\n",
    "\t\t\tweight_decay = weight_decay,\n",
    "\t\t)\n",
    "\t\tself.loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\t\tself.metrics = {\n",
    "\t\t\t\"loss\"     : self.loss_fn,\n",
    "\t\t\t\"accuracy\" : accuracy,\n",
    "\t\t\t\"precision\": precision,\n",
    "\t\t\t\"recall\"   : recall,\n",
    "\t\t\t\"f1\"       : f1,\n",
    "\t\t}\n",
    "\n",
    "\t\treturn self\n",
    "\n",
    "\tdef compute(self,\n",
    "\t\ty_pred: torch.Tensor,\n",
    "\t\ty_true: torch.Tensor,\n",
    "\t) -> dict[str, float]:\n",
    "\t\ty_prob = torch.sigmoid(y_pred)\n",
    "\n",
    "\t\treturn {name: metric(y_pred if name == \"loss\" else y_prob, y_true).item() for name, metric in self.metrics.items()}\n",
    "\n",
    "\tdef prune(self, train_dataset: TwitterDataset,\n",
    "\t\tmin_frequency: int = 1,\n",
    "\t\tmax_vocab_size: int | None = None,\n",
    "\t\tpad_token: str = \"<pad>\",\n",
    "\t\tunk_token: str = \"<unk>\",\n",
    "\t):\n",
    "\t\tif min_frequency > 1 or max_vocab_size is not None:\n",
    "\t\t\tfrequencies = Counter()\n",
    "\n",
    "\t\t\tfor text in track(train_dataset.data.Text, \"counting frequencies\".ljust(32), len(train_dataset.data)):\n",
    "\t\t\t\tfrequencies.update(preprocess_and_tokenize(text))\n",
    "\n",
    "\t\t\tself.model.embedding = self.model.embedding.prune_with_frequencies( frequencies,\n",
    "\t\t\t\tmin_frequency = min_frequency,\n",
    "\t\t\t\tmax_vocab_size = max_vocab_size,\n",
    "\t\t\t\tpad_token = pad_token,\n",
    "\t\t\t\tunk_token = unk_token,\n",
    "\t\t\t)\n",
    "\t\t\ttrain_dataset.transform.vocabulary = self.model.embedding.word2idx\n",
    "\n",
    "\t@preload\n",
    "\tdef fit(self,\n",
    "\t\ttrain_dataset: TwitterDataset,\n",
    "\t\tval_dataset  : TwitterDataset,\n",
    "\t\tepochs: int = 1,\n",
    "\t\tmin_frequency: int = 1,\n",
    "\t\tmax_vocab_size: int | None = None,\n",
    "\t**kwargs) -> dict[str, list[float]]:\n",
    "\t\tself.prune(train_dataset,\n",
    "\t\t\tmin_frequency = min_frequency,\n",
    "\t\t\tmax_vocab_size = max_vocab_size,\n",
    "\t\t\tpad_token = self.model.embedding.word2idx.pad_token,\n",
    "\t\t\tunk_token = self.model.embedding.word2idx.unk_token,\n",
    "\t\t)\n",
    "\t\tval_dataset.transform.vocabulary = self.model.embedding.word2idx\n",
    "\n",
    "\t\tmetrics = Counter(\n",
    "\t\t\tloss      = [], val_loss      = [],  # type: ignore\n",
    "\t\t\taccuracy  = [], val_accuracy  = [],  # type: ignore\n",
    "\t\t\tprecision = [], val_precision = [],  # type: ignore\n",
    "\t\t\trecall    = [], val_recall    = [],  # type: ignore\n",
    "\t\t\tf1        = [], val_f1        = [],  # type: ignore\n",
    "\t\t)\n",
    "\t\ttotal_loss = 0.\n",
    "\n",
    "\t\tprint()\n",
    "\t\tprint(f\"Training for {epochs} epochs with:\")\n",
    "\t\tprint(\n",
    "\t\t\tjson.dumps(kwargs,\n",
    "\t\t\t\tindent = 4,\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\ttrain_loader = torch.utils.data.DataLoader(train_dataset, **kwargs)\n",
    "\t\tassert train_loader.batch_size is not None\n",
    "\t\tbatches = len(train_dataset) // train_loader.batch_size\n",
    "\n",
    "\t\tself.model.train()\n",
    "\n",
    "\t\tprint()\n",
    "\n",
    "\t\twith Progress() as progress:\n",
    "\t\t\ttrain_task = progress.add_task(description = \"finished epoch ---/---\".ljust(32), total = epochs )\n",
    "\t\t\tbatch_task = progress.add_task(description = \"training loss -.------\".ljust(32), total = batches)\n",
    "\n",
    "\t\t\tfor epoch in range(epochs):\n",
    "\t\t\t\tprogress.reset(batch_task)\n",
    "\n",
    "\t\t\t\tself.model.train()\n",
    "\n",
    "\t\t\t\tfor batch_index, batch in enumerate(train_loader,\n",
    "\t\t\t\t\tstart = epoch * batches + 1,\n",
    "\t\t\t\t):\n",
    "\t\t\t\t\tx, y_true = batch\n",
    "\n",
    "\t\t\t\t\tself.optimizer.zero_grad()\n",
    "\t\t\t\t\ty_pred = self.model.forward(x)\n",
    "\t\t\t\t\tloss = self.loss_fn.forward(\n",
    "\t\t\t\t\t\ty_pred,\n",
    "\t\t\t\t\t\ty_true,\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\tloss.backward()\n",
    "\t\t\t\t\tself.optimizer.step()\n",
    "\n",
    "\t\t\t\t\ttotal_loss += loss.item()\n",
    "\n",
    "\t\t\t\t\tprogress.update(batch_task,\n",
    "\t\t\t\t\t\tdescription = f\"training loss {total_loss/batch_index:.6f}\".ljust(32),\n",
    "\t\t\t\t\t\ttotal = batches,\n",
    "\t\t\t\t\t\tadvance = 1,\n",
    "\t\t\t\t\t)\n",
    "\n",
    "\t\t\t\tmetrics.update({       name  : [metric] for name, metric in self.evaluate(train_dataset).items()})\n",
    "\t\t\t\tmetrics.update({f\"val_{name}\": [metric] for name, metric in self.evaluate(  val_dataset).items()})\n",
    "\n",
    "\t\t\t\tprogress.update(train_task,\n",
    "\t\t\t\t\tdescription = f\"finished epoch {epoch+1:3d}/{epochs:3d}\".ljust(32),\n",
    "\t\t\t\t\ttotal = epochs,\n",
    "\t\t\t\t\tadvance = 1,\n",
    "\t\t\t\t)\n",
    "\n",
    "\t\treturn dict(metrics)  # type: ignore\n",
    "\n",
    "\t@preload\n",
    "\t@torch.no_grad\n",
    "\tdef evaluate(self, test_dataset: TwitterDataset, **kwargs) -> dict[str, float]:\n",
    "\t\tbatch_size = kwargs.pop(\"batch_size\", len(test_dataset))\n",
    "\t\tloader = torch.utils.data.DataLoader(test_dataset,\n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t**kwargs)\n",
    "\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\ty_preds = []\n",
    "\t\ty_trues = []\n",
    "\n",
    "\t\tfor x, y_true in loader:\n",
    "\t\t\ty_pred = self.model(x)\n",
    "\n",
    "\t\t\ty_preds.append(y_pred)\n",
    "\t\t\ty_trues.append(y_true)\n",
    "\n",
    "\t\ty_pred = torch.cat(y_preds)\n",
    "\t\ty_true = torch.cat(y_trues)\n",
    "\n",
    "\t\treturn self.compute(\n",
    "\t\t\ty_pred,\n",
    "\t\t\ty_true,\n",
    "\t\t)\n",
    "\n",
    "\t@preload\n",
    "\t@torch.no_grad\n",
    "\tdef predict(self, dataset: TwitterDataset, **kwargs) -> torch.Tensor:\n",
    "\t\tloader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
    "\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\ty_pred = []\n",
    "\n",
    "\t\tfor batch in loader:\n",
    "\t\t\tx, _ = batch  # ignore labels\n",
    "\t\t\ty_pred.append((torch.sigmoid(self.model(x)) > 0.5).long())\n",
    "\n",
    "\t\treturn torch.cat(y_pred)\n",
    "\n",
    "\t@preload\n",
    "\t@torch.no_grad\n",
    "\tdef predict_proba(self, dataset: TwitterDataset, **kwargs) -> torch.Tensor:\n",
    "\t\tloader = torch.utils.data.DataLoader(dataset, **kwargs)\n",
    "\n",
    "\t\tself.model.eval()\n",
    "\n",
    "\t\ty_probs = []\n",
    "\n",
    "\t\tfor batch in loader:\n",
    "\t\t\tx, _ = batch  # ignore labels\n",
    "\t\t\ty_probs.append(torch.sigmoid(self.model(x)))\n",
    "\n",
    "\t\treturn torch.cat(y_probs)\n",
    "\n",
    "\n",
    "\tdef classification_report_str(self, dataset: TwitterDataset, **kwargs) -> str:\n",
    "\t\ty_true = torch.cat([y for _, y in torch.utils.data.DataLoader(dataset, **kwargs)])\n",
    "\t\ty_pred = self.predict(dataset, **kwargs)\n",
    "\n",
    "\t\treport = sklearn.metrics.classification_report(\n",
    "\t\t\ty_true.numpy(force = True),\n",
    "\t\t\ty_pred.numpy(force = True), digits = 6\n",
    "\t\t)\n",
    "\n",
    "\t\treturn str(report)\n",
    "\n",
    "\tdef roc_auc(self, dataset: TwitterDataset, **kwargs) -> float:\n",
    "\t\ty_true = torch.cat([y for _, y in torch.utils.data.DataLoader(dataset, **kwargs)])\n",
    "\t\ty_prob = self.predict_proba(dataset, **kwargs)\n",
    "\n",
    "\t\tscore = sklearn.metrics.roc_auc_score(\n",
    "\t\t\ty_true.numpy(force = True),\n",
    "\t\t\ty_prob.numpy(force = True),\n",
    "\t\t)\n",
    "\n",
    "\t\treturn float(score)\n",
    "\n",
    "\t@preload\n",
    "\tdef roc_curve(self, dataset: TwitterDataset, **kwargs) -> tuple[\n",
    "\t\tnp.ndarray,\n",
    "\t\tnp.ndarray,\n",
    "\t\tnp.ndarray,\n",
    "\t]:\n",
    "\t\ty_true = torch.cat([y for _, y in torch.utils.data.DataLoader(dataset, **kwargs)])\n",
    "\t\ty_prob = self.predict_proba(dataset, **kwargs)\n",
    "\n",
    "\t\treturn sklearn.metrics.roc_curve(\n",
    "\t\t\ty_true.numpy(force = True),\n",
    "\t\t\ty_prob.numpy(force = True),\n",
    "\t\t)\n",
    "\n",
    "\tdef plot_roc_curve(self, dataset: TwitterDataset, **kwargs):\n",
    "\t\tfpr, tpr, _ = self.roc_curve(dataset, **kwargs)\n",
    "\t\tauc = self.roc_auc(dataset, **kwargs)\n",
    "\n",
    "\t\tplt.figure(\n",
    "\t\t\tfigsize = (\n",
    "\t\t\t\t6,\n",
    "\t\t\t\t6,\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\t\tplt.plot(fpr, tpr, label=f\"ROC AUC = {auc:.4f}\")\n",
    "\t\tplt.plot(\n",
    "\t\t\t[0, 1],\n",
    "\t\t\t[0, 1], linestyle = \"--\", color = \"gray\"\n",
    "\t\t)\n",
    "\t\tplt.xlabel(\"False Positive Rate\")\n",
    "\t\tplt.ylabel(\"True Positive Rate\")\n",
    "\t\tplt.title(\"ROC Curve\")\n",
    "\t\tplt.legend()\n",
    "\t\tplt.grid(True)\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.savefig(\"roc_curve.png\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef plot_learning_curve(self, metrics: dict[str, list[float]],\n",
    "\t\tkeys: set[str] = {\n",
    "\t\t\t\"loss\",\n",
    "\t\t\t\"accuracy\",\n",
    "\t\t\t\"precision\",\n",
    "\t\t\t\"recall\",\n",
    "\t\t\t\"f1\",\n",
    "\t\t},\n",
    "\t):\n",
    "\t\tplt.figure(\n",
    "\t\t\tfigsize = (\n",
    "\t\t\t\t15,\n",
    "\t\t\t\t4,\n",
    "\t\t\t)\n",
    "\t\t)\n",
    "\n",
    "\t\tfor i, key in enumerate(keys, 1):\n",
    "\t\t\tplt.subplot(1, len(keys), i)\n",
    "\t\t\tplt.plot(metrics[label :=        key  ], label = label)\n",
    "\t\t\tplt.plot(metrics[label := f\"val_{key}\"], label = label)\n",
    "\t\t\tplt.xlabel(\"epoch\")\n",
    "\t\t\tplt.ylabel(key)\n",
    "\t\t\tplt.title(f\"{key} vs. epoch\")\n",
    "\t\t\tplt.legend()\n",
    "\t\t\tplt.grid(True)\n",
    "\n",
    "\t\tplt.tight_layout()\n",
    "\t\tplt.savefig(\"learning_curve.png\")\n",
    "\t\tplt.show()\n",
    "\n",
    "\tdef submit(self, dataset: TwitterDataset, *,\n",
    "\t\tsubmission_path: Path = Path(\"submission.csv\"),\n",
    "\t):\n",
    "\t\ty_pred = self.predict(dataset)\n",
    "\n",
    "\t\tsubmission = pd.DataFrame(\n",
    "\t\t\t{\n",
    "\t\t\t\t\"ID\": dataset.data.index,\n",
    "\t\t\t\t\"Label\": y_pred,\n",
    "\t\t\t}\n",
    "\t\t)\n",
    "\n",
    "\t\tsubmission.to_csv(submission_path,\n",
    "\t\t\tindex = False,\n",
    "\t\t\tencoding = \"utf-8\",\n",
    "\t\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37713e",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "This is where the magic happens. After a few manual trials the following settings were chosen. Optuna was not attempted, because running a single experiment already burns my GPU for several minutes, and I do not think I will benefit personally from super fine tuning my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a64997",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "\tseed = 42,\n",
    "\tepochs = 4,\n",
    "\tglove_dim = 300,\n",
    "\tfreeze = False,\n",
    "\tmax_len = 256,\n",
    "\tlearning_rate = 1e-3,\n",
    "\tweight_decay = 1e-1,\n",
    "\tdropout = 1e-1,\n",
    "#\tnum_layers = 3,\n",
    "#\thidden_dim = 300,\n",
    "\tlayer_dims = [\n",
    "\t\t150,\n",
    "\t\t100,\n",
    "\t\t75,\n",
    "\t]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f41c4",
   "metadata": {},
   "source": [
    "This is a helper function to prettify output of reports with fixed decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec43206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_metrics(metrics: dict[str, list[float]],\n",
    "\tdigits: int = 6,\n",
    ") -> dict[str, list[float]]:\n",
    "\treturn {k: [round(v, digits) for v in values] for k, values in metrics.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c64c537",
   "metadata": {},
   "source": [
    "This is the experiment:\n",
    "\n",
    "1. generate an embedding layer\n",
    "2. build a model and assign it an optimizer and a loss\n",
    "3. build a classifier using said model\n",
    "4. build a transformation layer that includes:\n",
    "   - preprocessing\n",
    "   - tokenization\n",
    "   - token indexing\n",
    "5. instantiate the datasets used\n",
    "   - a training dataset\n",
    "   - a validation dataset that we used for model selection\n",
    "6. train the model on the training dataset which also produces evaluation metrics\n",
    "7. produce reports for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae44c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(args.seed)\n",
    "\n",
    "embedding = Embedding.from_glove(args.glove_dim,\n",
    "\tfreeze = args.freeze,\n",
    ")\n",
    "model = TwitterModel(embedding,\n",
    "\thidden_dim = args.layer_dims or args.hidden_dim,\n",
    "\tnum_layers = args.num_layers,\n",
    "\tdropout    = args.dropout   ,\n",
    ")\n",
    "model.compile()\n",
    "\n",
    "with TwitterClassifier(model) as classifier:\n",
    "\tclassifier.compile(\n",
    "\t\tlearning_rate = args.learning_rate,\n",
    "\t\tweight_decay  = args.weight_decay ,\n",
    "\t)\n",
    "\n",
    "#\tGenerate a preprocessing and tokenization transform function for the dataset:\n",
    "\ttransform = TextTransform(embedding.word2idx,\n",
    "\t\tpreprocessor = Preprocessor(),\n",
    "\t\ttokenizer = Tokenizer(),\n",
    "\t\tmax_len = args.max_len,\n",
    "\t)\n",
    "\n",
    "#\tCreate datasets:\n",
    "\ttrain_data = TwitterDataset(\"train\", transform = transform)\n",
    "\tval_data   = TwitterDataset(\"val\"  , transform = transform)\n",
    "\ttest_data  = TwitterDataset(\"test\" , transform = transform)\n",
    "\n",
    "#\tTrain the model:\n",
    "\tmetrics = classifier.fit(\n",
    "\t\ttrain_data,\n",
    "\t\tval_data,\n",
    "\t\tepochs = args.epochs,\n",
    "\t\tbatch_size = int(math.log10(len(train_data) + len(val_data))) + 1,\n",
    "\t)\n",
    "\n",
    "#\tDump metrics to file:\n",
    "\twith open(\"sdi2200160.json\", \"w+\",\n",
    "\t\tencoding = \"utf-8\",\n",
    "\t) as file:\n",
    "\t\tjson.dump(round_metrics(metrics), file,\n",
    "\t\t\tindent = 4,\n",
    "\t\t)\n",
    "\n",
    "#\tGenerate report:\n",
    "\tprint()\n",
    "\tprint(classifier.classification_report_str(val_data))\n",
    "\tprint(\"ROC AUC:\", classifier.roc_auc(val_data))\n",
    "\tprint()\n",
    "\n",
    "\tclassifier.plot_roc_curve(val_data)\n",
    "\tclassifier.plot_learning_curve(metrics)\n",
    "\n",
    "#\tSubmit predictions:\n",
    "\tclassifier.submit(test_data,\n",
    "\t\tsubmission_path = Path(\"submission.csv\"),\n",
    "\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
